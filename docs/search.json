[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "TODO\n\n\n\n\n\n\n\n\n\n\n\n\nTTSBBC\n\n\n\nRShiny\n\n\nTriplex Target Sites\n\n\nSQL\n\n\n\ndescription of TTSBBC\n\n\n\nMaya\n\n\nJul 30, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html",
    "href": "projects/MNIST_classification_proj.html",
    "title": "Intro to ML Classification",
    "section": "",
    "text": "This project demonstrates a basic implementation of a multi-layer perceptron (MLP) using PyTorch. This project will create a classification model to identify handwritten numbers in the MNSIT dataset. The MNIST dataset is a large database of handwritten digits that is commonly used for training various image processing systems. It contains 60,000 training images and 10,000 testing images, each of which is a 28x28 grayscale image of a single digit from 0 to 9.\nIn this project, we will:\n\nLoad and preprocess the MNIST dataset\nDefine the MLP architecture using PyTorch\nTrain the neural network on the training data\nEvaluate the model’s performance on the validation data\nVisualize the results and analyze the model’s accuracy"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#data-transformation",
    "href": "projects/MNIST_classification_proj.html#data-transformation",
    "title": "Intro to ML Classification",
    "section": "Data Transformation",
    "text": "Data Transformation\nFirst we will define how we will transform all of the input data to train the model. First we will pull the bitmap into tensors that have separated the array of pixels into 3 color channels (RGB) with each pixel for that channel having a value from 0 to 255. Then we scale the values down to 0-1 and then normalized with a mean and standard deviation\n\ntransform = transforms.Compose(\n  [\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n  ]\n)"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#data-collection",
    "href": "projects/MNIST_classification_proj.html#data-collection",
    "title": "Intro to ML Classification",
    "section": "Data Collection",
    "text": "Data Collection\n\ntrainset = datasets.MNIST(\n  'data/MNIST/trainset', \n  download=True, \n  train=True, \n  transform=transform\n)\n\nvalset = datasets.MNIST(\n  'data/MNIST/valset', \n  download=True, \n  train=False, \n  transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(\n  trainset, \n  batch_size=64, \n  shuffle=True\n)\n\nvalloader = torch.utils.data.DataLoader(\n  valset, \n  batch_size=64, \n  shuffle=True\n)"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#explore-data-structure",
    "href": "projects/MNIST_classification_proj.html#explore-data-structure",
    "title": "Intro to ML Classification",
    "section": "Explore Data Structure",
    "text": "Explore Data Structure\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\nprint(images.shape)\nprint(labels.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\nThere are 64 images in each batch where each image is 28 pixels x 28 pixels\nThe labels have one dimension of 64 labels for the corresponding image!"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#view-image",
    "href": "projects/MNIST_classification_proj.html#view-image",
    "title": "Intro to ML Classification",
    "section": "View Image",
    "text": "View Image\n\nOne Image\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n\n\n\n\n\n\n\n\n\n\nMultiple Images\n\n# make a plot of 25 images in a 5x5 grid\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 5, 5\nfor i in range(1, cols * rows + 1):\n  img = images[i - 1].squeeze()\n  ax = figure.add_subplot(rows, cols, i)\n  ax.imshow(img, cmap=\"gray_r\")\n  # add title\n  ax.set_title(labels[i - 1].item(), y=1, pad=1.5)\n\n  # clean up subplot axes\n  ax.tick_params(axis='both', which='both', length=0)\n  ax.set_xticklabels([])\n  ax.set_yticklabels([])\n  # make axes thicker\n  for spine in ax.spines.values():\n    spine.set_edgecolor('black')\n    spine.set_linewidth(1.5)\n\nplt.show()"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#loss",
    "href": "projects/MNIST_classification_proj.html#loss",
    "title": "Intro to ML Classification",
    "section": "Loss",
    "text": "Loss\n\ncriterion = nn.NLLLoss()\nimages, labels = next(iter(trainloader))\nimages = images.view(images.shape[0], -1)\nlabels = labels\n\nlogps = model(images) #log probabilities\nloss = criterion(logps, labels) #calculate the NLL loss"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#move-to-cuda",
    "href": "projects/MNIST_classification_proj.html#move-to-cuda",
    "title": "Intro to ML Classification",
    "section": "Move to CUDA",
    "text": "Move to CUDA\n\nmodel.to(device)\nmodel_device = next(model.parameters()).device\nprint(f\"Model is on device: {model_device}\")\n\nModel is on device: cuda:0"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#train",
    "href": "projects/MNIST_classification_proj.html#train",
    "title": "Intro to ML Classification",
    "section": "Train",
    "text": "Train\n\noptimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\ntime0 = time()\nepochs = 15\n\nfor e in range(epochs):\n  running_loss = 0\n  for images, labels in trainloader:\n    # Flatten MNIST images into a 784 long vector and move to CUDA\n    images = images.view(images.shape[0], -1).to(device)\n    labels = labels.to(device)\n  \n    # Training pass\n    optimizer.zero_grad()\n    \n    output = model(images)\n    loss = criterion(output, labels)\n    \n    # This is where the model learns by backpropagating\n    loss.backward()\n    \n    # And optimizes its weights here\n    optimizer.step()\n    \n    running_loss += loss.item()\n  else:\n    print(f\"Epoch {e} - Training loss: {running_loss/len(trainloader)}\")\n    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n\nEpoch 0 - Training loss: 0.6279923344201752\n\nTraining Time (in minutes) = 0.19698334137598675\nEpoch 1 - Training loss: 0.2789794057766512\n\nTraining Time (in minutes) = 0.39831338326136273\nEpoch 2 - Training loss: 0.21601013075123462\n\nTraining Time (in minutes) = 0.5963752190272014\nEpoch 3 - Training loss: 0.17547521558898027\n\nTraining Time (in minutes) = 0.7978801568349202\nEpoch 4 - Training loss: 0.14620167161447248\n\nTraining Time (in minutes) = 1.0015810489654542\nEpoch 5 - Training loss: 0.12437244181010897\n\nTraining Time (in minutes) = 1.2041032910346985\nEpoch 6 - Training loss: 0.10704292071018137\n\nTraining Time (in minutes) = 1.4104810277620952\nEpoch 7 - Training loss: 0.09420193405970852\n\nTraining Time (in minutes) = 1.6121869246164957\nEpoch 8 - Training loss: 0.08485479259005646\n\nTraining Time (in minutes) = 1.8149256229400634\nEpoch 9 - Training loss: 0.076070878821026\n\nTraining Time (in minutes) = 2.0209671298662824\nEpoch 10 - Training loss: 0.06805198352638958\n\nTraining Time (in minutes) = 2.2193160971005756\nEpoch 11 - Training loss: 0.06324185459300685\n\nTraining Time (in minutes) = 2.4191258112589518\nEpoch 12 - Training loss: 0.057085515677071076\n\nTraining Time (in minutes) = 2.6251480023066205\nEpoch 13 - Training loss: 0.05319777697171849\n\nTraining Time (in minutes) = 2.8236159006754558\nEpoch 14 - Training loss: 0.0493274323494811\n\nTraining Time (in minutes) = 3.025309769312541"
  },
  {
    "objectID": "projects/MNIST_classification_proj.html#evaluation",
    "href": "projects/MNIST_classification_proj.html#evaluation",
    "title": "Intro to ML Classification",
    "section": "Evaluation",
    "text": "Evaluation\n\ndef view_classify(img, ps):\n    ''' \n    Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    ax2.set_yticklabels(np.arange(10))\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n    plt.tight_layout()\n\n\nmodel.to(\"cpu\")\nimages, labels = next(iter(valloader))\n\nimg = images[0].view(1, 784)\n\nwith torch.no_grad():\n    logps = model(img)\n\nps = torch.exp(logps)\nprobab = list(ps.numpy()[0])\nprint(\"Predicted Digit =\", probab.index(max(probab)))\nview_classify(img.view(1, 28, 28), ps)\n\nPredicted Digit = 4\n\n\n\n\n\n\n\n\n\nAccuracy for each class in classification.\n\ncorrect_count, all_count = 0, 0\nfor images,labels in valloader:\n  for i in range(len(labels)):\n    img = images[i].view(1, 784)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.numpy()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.numpy()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))\n\nNumber Of Images Tested = 10000\n\nModel Accuracy = 0.9753\n\n\n\n# assess accuracy by class\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\n\nwith torch.no_grad():\n  for images, labels in valloader:\n    images = images.view(images.shape[0], -1)\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n    c = (predicted == labels).squeeze()\n    for i in range(len(labels)):\n      label = labels[i]\n      class_correct[label] += c[i].item()\n      class_total[label] += 1\n\n# Calculate average accuracy\naverage_accuracy = correct_count/all_count * 100\n\n# Plot accuracy by class\nfig, ax = plt.subplots()\nclasses = list(range(10))\naccuracies = [100 * class_correct[i] / class_total[i] if class_total[i] &gt; 0 else 0 for i in classes]\n\nax.bar(classes, accuracies, color='blue')\nax.axhline(y=average_accuracy, color='red', linestyle='--', label=f'Average Accuracy: {average_accuracy:.2f}%')\n\nax.set_xlabel('Class')\nax.set_ylabel('Accuracy (%)')\nax.set_title('Accuracy by Class')\nax.set_xticks(classes)\nax.set_xticklabels(classes)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# find which classes were most often misclassified as the other classes\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Initialize the prediction and true label lists\nall_preds = []\nall_labels = []\n\n# Collect all predictions and true labels\nwith torch.no_grad():\n  for images, labels in valloader:\n    images = images.view(images.shape[0], -1)\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n    all_preds.extend(predicted.numpy())\n    all_labels.extend(labels.numpy())\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_preds)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()"
  },
  {
    "objectID": "projects/sql_intro_proj.html",
    "href": "projects/sql_intro_proj.html",
    "title": "Intro to SQL with DuckDB",
    "section": "",
    "text": "Hi, in this project we will be creating a database using DuckDB and querying it using SQL. We will use the nycflights13 package to provide the data for our database. This guide will walk you through the steps to set up the database, connect to it, and perform various queries.\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(duckdb)"
  },
  {
    "objectID": "projects/sql_intro_proj.html#create-the-query-connection",
    "href": "projects/sql_intro_proj.html#create-the-query-connection",
    "title": "Intro to SQL with DuckDB",
    "section": "Create the query connection",
    "text": "Create the query connection\nHere we establish the connection to the locally hosted DuckDB instance. As a data consumer we will only read data so read_only will be set to true. This connection will power all of our future analytics.\n\ncon_flights &lt;- DBI::dbConnect(\n  drv = duckdb::duckdb(),\n  dbdir = \"./data/flights_db.duckdb\",\n  read_only = TRUE\n)"
  },
  {
    "objectID": "projects/sql_intro_proj.html#query-with-a-sql-chunk",
    "href": "projects/sql_intro_proj.html#query-with-a-sql-chunk",
    "title": "Intro to SQL with DuckDB",
    "section": "Query with a SQL chunk",
    "text": "Query with a SQL chunk\nIn quarto we can use SQL to directly query ### Find the Carriers\n\nSELECT * FROM airlines LIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\ncarrier\nname\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\nEV\nExpressJet Airlines Inc.\n\n\nF9\nFrontier Airlines Inc.\n\n\nFL\nAirTran Airways Corporation\n\n\nHA\nHawaiian Airlines Inc.\n\n\nMQ\nEnvoy Air"
  },
  {
    "objectID": "projects/sql_intro_proj.html#query-with-an-r-chunk",
    "href": "projects/sql_intro_proj.html#query-with-an-r-chunk",
    "title": "Intro to SQL with DuckDB",
    "section": "Query with an R chunk",
    "text": "Query with an R chunk\n\nList Tables\nUsing DBI commands we are able to send commands to DuckDB. Here we can list the tables.\n\ntables &lt;- dbListTables(con_flights)\nprint(tables)\n\n[1] \"airlines\" \"airports\" \"flights\"  \"planes\"   \"weather\" \n\n\n\n\nTop of ‘flights’\nWe can also list the first 10 entries in the flights table.\n\nresult &lt;- DBI::dbGetQuery(\n  con_flights, \n\"\nSELECT * \nFROM flights \nLIMIT 10\n\"\n)\nprint(result)\n\n   year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1  2013     1   1      517            515         2      830            819\n2  2013     1   1      533            529         4      850            830\n3  2013     1   1      542            540         2      923            850\n4  2013     1   1      544            545        -1     1004           1022\n5  2013     1   1      554            600        -6      812            837\n6  2013     1   1      554            558        -4      740            728\n7  2013     1   1      555            600        -5      913            854\n8  2013     1   1      557            600        -3      709            723\n9  2013     1   1      557            600        -3      838            846\n10 2013     1   1      558            600        -2      753            745\n   arr_delay carrier flight tailnum origin dest air_time distance hour minute\n1         11      UA   1545  N14228    EWR  IAH      227     1400    5     15\n2         20      UA   1714  N24211    LGA  IAH      227     1416    5     29\n3         33      AA   1141  N619AA    JFK  MIA      160     1089    5     40\n4        -18      B6    725  N804JB    JFK  BQN      183     1576    5     45\n5        -25      DL    461  N668DN    LGA  ATL      116      762    6      0\n6         12      UA   1696  N39463    EWR  ORD      150      719    5     58\n7         19      B6    507  N516JB    EWR  FLL      158     1065    6      0\n8        -14      EV   5708  N829AS    LGA  IAD       53      229    6      0\n9         -8      B6     79  N593JB    JFK  MCO      140      944    6      0\n10         8      AA    301  N3ALAA    LGA  ORD      138      733    6      0\n             time_hour\n1  2013-01-01 10:00:00\n2  2013-01-01 10:00:00\n3  2013-01-01 10:00:00\n4  2013-01-01 10:00:00\n5  2013-01-01 11:00:00\n6  2013-01-01 10:00:00\n7  2013-01-01 11:00:00\n8  2013-01-01 11:00:00\n9  2013-01-01 11:00:00\n10 2013-01-01 11:00:00\n\nDBI::dbDisconnect(con_flights)"
  },
  {
    "objectID": "projects/sql_intro_proj.html#count-the-flights-of-each-plane",
    "href": "projects/sql_intro_proj.html#count-the-flights-of-each-plane",
    "title": "Intro to SQL with DuckDB",
    "section": "Count the flights of each plane",
    "text": "Count the flights of each plane\nFor each plane, denoted by their tailnum, I counted the number of flights each plane went on and the average distance of the flights it took\n\n# get the number of flights each plane went on\nresult &lt;- DBI::dbGetQuery(\n  con_flights, \n\"\nSELECT COUNT(all_tailnum.tailnum) AS 'n_flights', AVG(all_tailnum.distance) AS 'mean_dist', all_tailnum.tailnum\nFROM(\n  SELECT flights.tailnum, flights.distance\n  FROM flights\n) AS all_tailnum\nGROUP BY all_tailnum.tailnum\nORDER BY n_flights DESC\n\"\n)\nhead(result)\n\n  n_flights mean_dist tailnum\n1       575  558.6052  N725MQ\n2       513  545.8908  N722MQ\n3       507  537.6272  N723MQ\n4       486  541.9733  N711MQ\n5       483  549.4762  N713MQ\n6       427  529.8806  N258JB\n\n\n\nggplot(result) +\n  aes(\n    x = mean_dist,\n    y = n_flights\n  ) +\n  geom_point() +\n  theme_classic()"
  },
  {
    "objectID": "projects/sql_intro_proj.html#find-the-fleet-compositions",
    "href": "projects/sql_intro_proj.html#find-the-fleet-compositions",
    "title": "Intro to SQL with DuckDB",
    "section": "Find the fleet compositions",
    "text": "Find the fleet compositions\nHere we pulled the information about each airline carrier’s fleet of planes they flew a flight in this dataset. We assessed their manufacturer and model and counted the number of each of the types of planes by distinct tailnum.\n\nresult &lt;- dbGetQuery(\n  con_flights, \n\"\nSELECT airlines.name, manufacturer, model, COUNT(distinct_tailnums.tailnum)\nFROM (\n  SELECT DISTINCT flights.tailnum, flights.carrier, manufacturer, model\n  FROM flights\n  LEFT JOIN planes\n  ON flights.tailnum = planes.tailnum\n) AS distinct_tailnums\nLEFT JOIN airlines\nON distinct_tailnums.carrier = airlines.carrier\nGROUP BY airlines.name, manufacturer, model\n\"\n)\nhead(result)\n\n                    name manufacturer     model\n1   Delta Air Lines Inc.       BOEING   757-231\n2 American Airlines Inc.       CESSNA       550\n3  United Air Lines Inc.       AIRBUS  A319-131\n4         Virgin America       AIRBUS  A320-214\n5 Southwest Airlines Co.       BOEING   737-3H4\n6 American Airlines Inc.   MARZ BARRY KITFOX IV\n  count(distinct_tailnums.tailnum)\n1                                8\n2                                1\n3                                5\n4                               43\n5                              105\n6                                1\n\n\n\nresult %&gt;%\n  # remove NAs\n  filter(!is.na(manufacturer)) %&gt;%\n  filter(!is.na(model)) %&gt;%\n  # Plot only United Air Lines Inc.\n  filter(name == \"United Air Lines Inc.\") %&gt;%\n  rename(n_planes = \"count(distinct_tailnums.tailnum)\") %&gt;%\n  ggplot() + \n  aes(\n    x = manufacturer,\n    y = model,\n    fill = n_planes\n  ) +\n  geom_tile() + \n  # Used to plot all airlines at once\n  # facet_wrap(~name, scales = \"free\") +\n  theme_classic() +\n  theme(\n    axis.text.x = element_text(angle = 90)\n  )"
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html",
    "href": "projects/ExploratoryDataAnalysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The data used in this project was collected from the USDA’s Atlas of Rural and Small-Town America. Located at: https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/\nThese datasets were interesting to me as they contained data for the entire nation, but each observation was a small local. So this dataset gives a good representation of the country by area, population was not accounted for in this project. The unique county classifications and employment measurements sparked my curiosity. The other variables I acquired from these datasets were of poverty and income, as I was curious if they were correlated with a certain type of employment in a job sector."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#introduction",
    "href": "projects/ExploratoryDataAnalysis.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The data used in this project was collected from the USDA’s Atlas of Rural and Small-Town America. Located at: https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/\nThese datasets were interesting to me as they contained data for the entire nation, but each observation was a small local. So this dataset gives a good representation of the country by area, population was not accounted for in this project. The unique county classifications and employment measurements sparked my curiosity. The other variables I acquired from these datasets were of poverty and income, as I was curious if they were correlated with a certain type of employment in a job sector."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#importing-data-and-packages",
    "href": "projects/ExploratoryDataAnalysis.html#importing-data-and-packages",
    "title": "Exploratory Data Analysis",
    "section": "Importing data and packages:",
    "text": "Importing data and packages:\n\nPackages\n\nlibrary(tidyverse)\n\n\n\nRead Data\n\njobs &lt;- read_csv(\"data/ExploratoryDataAnalysis/Jobs.csv\")\ncounty &lt;- read_csv(\"data/ExploratoryDataAnalysis/County Classifications.csv\")\nincome &lt;- read_csv(\"data/ExploratoryDataAnalysis/Income.csv\")\n\n\n\nSelect applicable variables\n\ncounty2 &lt;- county %&gt;% \n  select(FIPS=FIPStxt,State, County,Type_2015_Update) %&gt;% \n  mutate(type=as.factor(Type_2015_Update)) %&gt;% \n  mutate( County_type=\n            recode(Type_2015_Update,\n                   \"0\"=\"Nonspecialized\",\n                   \"1\"=\"farm-dependent\", \n                   \"2\"=\"Mining-dependent\", \n                   \"3\"=\"Manufacturing-dependent\", \n                   \"4\"=\"FedStagovernment-dependent\", \n                   \"5\"=\"Recreation\" ) ) %&gt;% \n  select(-Type_2015_Update) %&gt;% \n  mutate( County_type=as.factor(County_type) )\n\nThe county dataset provided a classification system of US counties with measures(from 2015) of earnings and employment to create county types. I then recoded the discrete numeric values into a readable categorical variable.\n\njobs2 &lt;- jobs %&gt;% \n  select(FIPS,\n         State, \n         County,\n         contains(\"UnempRate\"),\n         contains(\"PctEmp\"),\n         -contains(\"PctEmpChange\")) %&gt;% \n  mutate( UnemploymentRate=\n            (UnempRate2018+\n             UnempRate2017+\n             UnempRate2016+\n             UnempRate2015+\n             UnempRate2014)/5 ) %&gt;% \n  select(-contains(\"20\"))\n\nglimpse(jobs2)\n\nRows: 3,278\nColumns: 14\n$ FIPS                &lt;chr&gt; \"00000\", \"01000\", \"01001\", \"01003\", \"01005\", \"0100…\n$ State               &lt;chr&gt; \"US\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"A…\n$ County              &lt;chr&gt; \"United States\", \"Alabama\", \"Autauga\", \"Baldwin\", …\n$ PctEmpAgriculture   &lt;dbl&gt; 1.2845839, 1.0695774, 0.5513182, 0.9563178, 4.4036…\n$ PctEmpMining        &lt;dbl&gt; 0.54431821, 0.41893227, 0.33161996, 0.37160389, 0.…\n$ PctEmpConstruction  &lt;dbl&gt; 6.465190, 6.537221, 6.002321, 8.269525, 6.250000, …\n$ PctEmpManufacturing &lt;dbl&gt; 10.181289, 14.243842, 12.945614, 9.119824, 23.8876…\n$ PctEmpTrade         &lt;dbl&gt; 13.923130, 14.325943, 11.648151, 16.800351, 13.405…\n$ PctEmpTrans         &lt;dbl&gt; 5.227259, 5.366571, 6.984745, 4.925090, 6.915138, …\n$ PctEmpInformation   &lt;dbl&gt; 2.0716835, 1.5777856, 1.6290831, 1.3696870, 0.2408…\n$ PctEmpFIRE          &lt;dbl&gt; 6.557098, 5.537273, 6.259327, 7.398880, 3.967890, …\n$ PctEmpServices      &lt;dbl&gt; 49.11018, 45.48632, 43.39662, 45.72227, 33.71560, …\n$ PctEmpGovt          &lt;dbl&gt; 4.635271, 5.436537, 10.251202, 5.066450, 7.098624,…\n$ UnemploymentRate    &lt;dbl&gt; 4.94, 5.40, 4.72, 4.92, 7.72, 5.70, 4.88, 6.60, 6.…\n\n\nThe jobs dataset provided, for each county/FIPS code, a percent of the civilian labor force 16 and over employed in a given job sector as a multi-year average from 2014 to 2018. The dataset also provided yearly unemployment rates over 2014-2018. I created a variable as the multiyear average of unemployment rate from 2014-2018 for exploratory purposes.\nFrom the documentation of the data the categorical Job Sectors are as follows\n\n\n\n\n\n\n\nJob Sector Abbreviation\nMeaning\n\n\n\n\nAgriculture\nagriculture, forestry, fishing, and hunting\n\n\nMining\nmining, quarrying, oil and gas extraction\n\n\nConstruction\nConstruction\n\n\nManufacturing\nManufacturing\n\n\nTrade\nwholesale and retail trade\n\n\nTrans\ntransportation, warehousing and utilities\n\n\nInformation\ninformation\n\n\nFIRE\nfinance and insurance, and real estate and rental and leasing\n\n\nServices\nservices\n\n\nGovt\npublic administration\n\n\n\n\nincome2 &lt;- income %&gt;% \n  select(FIPS,\n         State,\n         County,\n         PerCapitaInc, \n         PerDeepPov=Deep_Pov_All,\n         PerChildrenDeepPov=Deep_Pov_Children)\n\nglimpse(income2)\n\nRows: 3,278\nColumns: 6\n$ FIPS               &lt;chr&gt; \"00000\", \"01000\", \"01001\", \"01003\", \"01005\", \"01007…\n$ State              &lt;chr&gt; \"US\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL…\n$ County             &lt;chr&gt; \"United States\", \"Alabama\", \"Autauga\", \"Baldwin\", \"…\n$ PerCapitaInc       &lt;dbl&gt; 32621, 26846, 29372, 31203, 18461, 20199, 22656, 20…\n$ PerDeepPov         &lt;dbl&gt; 6.249590, 7.611623, 6.142609, 4.482528, 12.749387, …\n$ PerChildrenDeepPov &lt;dbl&gt; 8.598276, 11.591313, 8.910594, 6.214526, 26.709797,…\n\n\nThe income dataset provided, for each county/FIPS code, Per Capita Income, Deep Poverty, and Deep Child Poverty over the same multi-year average (2014 to 2018).\n\n\nTidy data\n\njobs2_tidy &lt;- jobs2 %&gt;% \n  pivot_longer(4:13, \n               names_to=\"PctEmployed_TYPE\", \n               values_to=\"PercentEmployed\") %&gt;% \n  separate(PctEmployed_TYPE, \n           into=c(\"rm\",\"PctEmployed_TYPE\"),\n           sep=6 ) %&gt;% \n  select(-rm) %&gt;% \n  mutate( PctEmployed_TYPE=as.factor(PctEmployed_TYPE) )\n\njobs was in a wide format for percent employed in each job sector. So, a pivot_longer() was done to create a categorical variable for the type of percent employed in each sector. This was saved as a separate value so that analysis can be done with either format."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#merge-datasets",
    "href": "projects/ExploratoryDataAnalysis.html#merge-datasets",
    "title": "Exploratory Data Analysis",
    "section": "Merge datasets:",
    "text": "Merge datasets:\n\nFind overlap of datasets\n\nCount uinque observations\n\njobs2_tidy %&gt;% \n  select(FIPS, State, County) %&gt;% \n  summarise_all(n_distinct)\n\n# A tibble: 1 × 3\n   FIPS State County\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  3278    53   1947\n\ncounty2 %&gt;% \n  select(FIPS, State, County) %&gt;% \n  summarise_all(n_distinct)\n\n# A tibble: 1 × 3\n   FIPS State County\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  3225    52   1913\n\nincome2 %&gt;% \n  select(FIPS, State, County) %&gt;% \n  summarise_all(n_distinct)\n\n# A tibble: 1 × 3\n   FIPS State County\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  3278    53   1947\n\n\nHere there is not the same number of unique observations for all datasets, so lets find which values are missing.\n\n\nFind differences in FIPS\n\nanti_join(jobs2_tidy, county2, by=\"FIPS\") %&gt;% \n  group_by(FIPS)\n\n# A tibble: 530 × 6\n# Groups:   FIPS [53]\n   FIPS  State County        UnemploymentRate PctEmployed_TYPE PercentEmployed\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1 00000 US    United States             4.94 Agriculture                1.28 \n 2 00000 US    United States             4.94 Mining                     0.544\n 3 00000 US    United States             4.94 Construction               6.47 \n 4 00000 US    United States             4.94 Manufacturing             10.2  \n 5 00000 US    United States             4.94 Trade                     13.9  \n 6 00000 US    United States             4.94 Trans                      5.23 \n 7 00000 US    United States             4.94 Information                2.07 \n 8 00000 US    United States             4.94 FIRE                       6.56 \n 9 00000 US    United States             4.94 Services                  49.1  \n10 00000 US    United States             4.94 Govt                       4.64 \n# ℹ 520 more rows\n\nanti_join(income2, county2, by=\"FIPS\") %&gt;% \n  group_by(FIPS)\n\n# A tibble: 53 × 6\n# Groups:   FIPS [53]\n   FIPS  State County               PerCapitaInc PerDeepPov PerChildrenDeepPov\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                       &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 00000 US    United States               32621       6.25               8.60\n 2 01000 AL    Alabama                     26846       7.61              11.6 \n 3 02000 AK    Alaska                      35874       4.94               6.31\n 4 04000 AZ    Arizona                     29265       7.50              10.2 \n 5 05000 AR    Arkansas                    25635       7.25              10.3 \n 6 06000 CA    California                  35021       6.25               8.04\n 7 08000 CO    Colorado                    36415       4.97               5.75\n 8 09000 CT    Connecticut                 43056       4.67               6.29\n 9 10000 DE    Delaware                    33989       5.44               6.97\n10 11000 DC    District of Columbia        53321       9.20              13.6 \n# ℹ 43 more rows\n\n\nThe county dataset is the only dataset missing observations (53) which are for the state/federal cumulative statistics.\n\n\nFind missing data\n\njobs2_tidy %&gt;%  \n  filter(!complete.cases(jobs2_tidy)) %&gt;% \n  group_by(FIPS) %&gt;% \n  slice(0:1) \n\n# A tibble: 7 × 6\n# Groups:   FIPS [7]\n  FIPS  State County           UnemploymentRate PctEmployed_TYPE PercentEmployed\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                       &lt;dbl&gt; &lt;fct&gt;                      &lt;dbl&gt;\n1 02010 AK    Aleutian Islands             NA   Agriculture                NA   \n2 02201 AK    Prince of Wales…             NA   Agriculture                NA   \n3 02232 AK    Skagway-Hoonah-…             NA   Agriculture                NA   \n4 02280 AK    Wrangell-Peters…             NA   Agriculture                NA   \n5 15005 HI    Kalawao                      NA   Agriculture                 1.75\n6 35039 NM    Rio Arriba                    7.1 Agriculture                NA   \n7 51515 VA    Bedford                      NA   Agriculture                NA   \n\ncounty2 %&gt;% \n  filter(!complete.cases(county2)) %&gt;% \n  group_by(FIPS) %&gt;% \n  slice(0:1)\n\n# A tibble: 82 × 5\n# Groups:   FIPS [82]\n   FIPS  State County                          type  County_type\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                           &lt;fct&gt; &lt;fct&gt;      \n 1 02010 AK    Aleutian Islands                &lt;NA&gt;  &lt;NA&gt;       \n 2 02201 AK    Prince of Wales-Outer Ketchikan &lt;NA&gt;  &lt;NA&gt;       \n 3 02232 AK    Skagway-Hoonah-Angoon           &lt;NA&gt;  &lt;NA&gt;       \n 4 02280 AK    Wrangell-Petersburg             &lt;NA&gt;  &lt;NA&gt;       \n 5 72001 PR    Adjuntas                        &lt;NA&gt;  &lt;NA&gt;       \n 6 72003 PR    Aguada                          &lt;NA&gt;  &lt;NA&gt;       \n 7 72005 PR    Aguadilla                       &lt;NA&gt;  &lt;NA&gt;       \n 8 72007 PR    Aguas Buenas                    &lt;NA&gt;  &lt;NA&gt;       \n 9 72009 PR    Aibonito                        &lt;NA&gt;  &lt;NA&gt;       \n10 72011 PR    Añasco                          &lt;NA&gt;  &lt;NA&gt;       \n# ℹ 72 more rows\n\nincome2 %&gt;% \n  filter(!complete.cases(income2)) %&gt;% \n  group_by(FIPS) %&gt;% \n  slice(0:1)\n\n# A tibble: 7 × 6\n# Groups:   FIPS [7]\n  FIPS  State County                  PerCapitaInc PerDeepPov PerChildrenDeepPov\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n1 02010 AK    Aleutian Islands                  NA      NA                    NA\n2 02201 AK    Prince of Wales-Outer …           NA      NA                    NA\n3 02232 AK    Skagway-Hoonah-Angoon             NA      NA                    NA\n4 02280 AK    Wrangell-Petersburg               NA      NA                    NA\n5 15005 HI    Kalawao                        47709       9.09                 NA\n6 35039 NM    Rio Arriba                        NA      NA                    NA\n7 51515 VA    Bedford                           NA      NA                    NA\n\n\nThere are 6 incomplete observations in jobs. 4 are in AK, 1 in NM, 1 in VA. These 6 observations are also missing from income. In addition to the priorly mentioned 6 missing observations in income, there is also 1 in HI that is missing. The 4 observations in AK that are missing from the prior 2 datasets are also missing for county, but county is also missing all the observations for PR.\n\n\n\nMerge datasets\n\ndata &lt;- county2 %&gt;% full_join(jobs2, by=c(\"FIPS\",\"State\",\"County\")) %&gt;% full_join(income2,by=c(\"FIPS\",\"State\",\"County\"))\ndatatidy &lt;- county2 %&gt;% full_join(jobs2_tidy, by=c(\"FIPS\",\"State\",\"County\")) %&gt;% full_join(income2,by=c(\"FIPS\",\"State\",\"County\"))\n\nA full join by the county’s FIPS code was picked to preserve all the available data and remove entries with missing/incomplete information. These missing points should not have too much leverage later in the exploratory analysis."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#summary-statistics",
    "href": "projects/ExploratoryDataAnalysis.html#summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics:",
    "text": "Summary Statistics:\n\nPercent values of employment sum to 100\n\ndata %&gt;% \n  na.omit() %&gt;% \n  mutate( pct_tot=(PctEmpAgriculture+\n                     PctEmpMining+\n                     PctEmpConstruction+\n                     PctEmpManufacturing+\n                     PctEmpTrade+\n                     PctEmpTrans+\n                     PctEmpInformation+\n                     PctEmpFIRE+\n                     PctEmpServices+\n                     PctEmpGovt) ) %&gt;% \n  select (FIPS,pct_tot) %&gt;% \n  mutate( is100 = (round(pct_tot,5)==100) ) %&gt;% \n  summarise(sum_to_100=sum(is100), unequal_to_100=sum(!is100))\n\n# A tibble: 1 × 2\n  sum_to_100 unequal_to_100\n       &lt;int&gt;          &lt;int&gt;\n1       3140              0\n\n\nFor the complete observations, the percent employed for each job sector sums to 100, confirming that there are only complete observations.\n\n\nThe most common county type by state\n\ndata %&gt;% \n  na.omit %&gt;% \n  group_by(State, County_type) %&gt;% \n  count() %&gt;% \n  group_by(State) %&gt;% \n  mutate(MajorityOfCountiesType=County_type,totalCountiesInState=sum(n), \n         majority_county_count=n) %&gt;% \n  select(-n,-County_type) %&gt;%  \n  group_by(State) %&gt;% \n  filter( majority_county_count==max(majority_county_count) ) %&gt;% \n  mutate(percent_county_type=round(majority_county_count/totalCountiesInState,4)*100) %&gt;% \n  arrange(desc(percent_county_type))\n\n# A tibble: 55 × 5\n# Groups:   State [51]\n   State MajorityOfCountiesType     totalCountiesInState majority_county_count\n   &lt;chr&gt; &lt;fct&gt;                                     &lt;int&gt;                 &lt;int&gt;\n 1 DC    FedStagovernment-dependent                    1                     1\n 2 CT    Nonspecialized                                8                     7\n 3 NJ    Nonspecialized                               21                    16\n 4 ND    farm-dependent                               53                    35\n 5 MA    Nonspecialized                               14                     9\n 6 NE    farm-dependent                               93                    58\n 7 SD    farm-dependent                               66                    41\n 8 MS    Nonspecialized                               82                    50\n 9 RI    Nonspecialized                                5                     3\n10 VT    Recreation                                   14                     8\n# ℹ 45 more rows\n# ℹ 1 more variable: percent_county_type &lt;dbl&gt;\n\n\nThe above table illustrates each state’s total number of counties, and the type of their most prevalent county, if there are ties for the majority then all of the top ranking for that state are present. The table is arranged by percent of the state’s counties that are of the given majority type. Most of the states have a majority type of Nonspecialized or Recreation. There are very few states that have a majority of their counties devoted to Mining, Manufacturing or Government. The larger states (the ones with the most counties) tend to be mostly Nonspecialized. One of the most interesting find in the table is that Hawaii has 4 counties, 2 are government and the other 2 are recreation, so all of Hawaii’s counties are present in the above table.\n\n\nSummary Statistics for nonEmployment variables\n\ndatatidy %&gt;% \n  na.omit() %&gt;% \n  select(-PercentEmployed) %&gt;% \n  summarise_if(is.numeric, \n               c(\"sd\"=\"sd\",\n                 \"mean\"=\"mean\",\n                 \"med\"=\"median\",\n                 \"max\"=\"max\",\n                 \"min\"=\"min\") ) %&gt;% \n  pivot_longer(1:20, values_to=\"statistic\") %&gt;% \n  separate(name,into=c(\"Variable\",\"stat_type\"), sep=\"_\") %&gt;% \n  arrange(Variable)\n\n# A tibble: 20 × 3\n   Variable           stat_type statistic\n   &lt;chr&gt;              &lt;chr&gt;         &lt;dbl&gt;\n 1 PerCapitaInc       sd          6502.  \n 2 PerCapitaInc       mean       27030.  \n 3 PerCapitaInc       med        26244.  \n 4 PerCapitaInc       max        72832   \n 5 PerCapitaInc       min        10148   \n 6 PerChildrenDeepPov sd             5.99\n 7 PerChildrenDeepPov mean           9.55\n 8 PerChildrenDeepPov med            8.54\n 9 PerChildrenDeepPov max           50.5 \n10 PerChildrenDeepPov min            0   \n11 PerDeepPov         sd             3.34\n12 PerDeepPov         mean           6.68\n13 PerDeepPov         med            6.09\n14 PerDeepPov         max           33.2 \n15 PerDeepPov         min            0   \n16 UnemploymentRate   sd             1.81\n17 UnemploymentRate   mean           5.15\n18 UnemploymentRate   med            4.92\n19 UnemploymentRate   max           22.2 \n20 UnemploymentRate   min            1.76\n\n\nThe above table depicts the standard deviation, mean, median, maximium, and minium for all nonEmployment variables. The Per Capita Income statistics show that the maximium value is about 7 standard deviations from the mean, while the minimum is only about 2.5 standard deviations below the mean which will be explored further in the data visualization section. Both Unemployment Rate and the two measures of poverty also have maximums greatly above the means, for their given standard deviations. Given this information and the relations of mean and median then it is reasonable to predict that these variables will be right skewed.\n\ndatatidy %&gt;% \n  na.omit() %&gt;% \n  select(FIPS,County,County_type,PctEmployed_TYPE,PercentEmployed) %&gt;% \n  group_by(County_type, PctEmployed_TYPE) %&gt;% \n  summarise_if(is.numeric, c(\"mean\"=\"mean\") ) %&gt;% \n  mutate(mean=round(mean,2)) %&gt;% \n  pivot_wider(names_from=\"PctEmployed_TYPE\", values_from=\"mean\") %&gt;% \n  print()\n\n# A tibble: 6 × 11\n# Groups:   County_type [6]\n  County_type     Agriculture Construction  FIRE  Govt Information Manufacturing\n  &lt;fct&gt;                 &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 farm-dependent        15.2          7.46  4.12  5.26        1.25          9.56\n2 FedStagovernme…        3.66         6.65  4.4   8.54        1.36          8.84\n3 Manufacturing-…        3.41         6.81  3.98  4.33        1.19         22.0 \n4 Mining-depende…        4.67         7.82  3.66  5.68        1.18          7   \n5 Nonspecialized         3.1          7.25  5.03  5.21        1.49         12.4 \n6 Recreation             3.53         8.72  4.98  5.6         1.48          8.74\n# ℹ 4 more variables: Mining &lt;dbl&gt;, Services &lt;dbl&gt;, Trade &lt;dbl&gt;, Trans &lt;dbl&gt;\n\n\nThe above table illustrates the mean percentage employed in each job sector for each different county type. The Services sector appears to always have a large majority of the employment regardless of the county type. And, for some job sectors if they are of the similar/related to the county type they have a larger mean percent employed, for instance Agriculture has a higher value for the farm-dependent counties."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#data-visualization",
    "href": "projects/ExploratoryDataAnalysis.html#data-visualization",
    "title": "Exploratory Data Analysis",
    "section": "Data Visualization:",
    "text": "Data Visualization:\n\nHeatmap\n\ncormat &lt;- data %&gt;% \n  select_if(is.numeric) %&gt;% \n  cor(use=\"pair\")\n\ntidycor &lt;- cormat %&gt;% \n  as.data.frame %&gt;% \n  rownames_to_column(\"var1\") %&gt;%  \n  pivot_longer(-1,names_to=\"var2\",values_to=\"correlation\")\n\n\nggplot(tidycor) + \n  aes(x=var1,y=var2, fill=correlation) +\n  theme_bw() +\n  theme( axis.text.x = element_text(angle = 30, vjust = 1, hjust=1)) +\n  labs(title = \"Correlation Matrix for all variables\") +\n  geom_tile(color = \"white\",\n            lwd = 0.5,\n            linetype = 1) +\n  coord_fixed() + \n  scale_fill_viridis_c(option = \"plasma\") +\n  geom_text(aes(label = round(correlation,2)), color=\"white\",size=2)\n\n\n\n\n\n\n\n\n\n\nPlots\n\nStacked Barplot\n\ndatatidy %&gt;% filter(complete.cases(datatidy)) %&gt;% ggplot(aes(x=County_type,y=PercentEmployed,fill=PctEmployed_TYPE)) +\n  labs(title=\"Average Percent Employed in each Job Sector for each given County Type\")+\n  geom_bar(stat=\"summary\", fun=\"median\", position=\"fill\") +\n  theme(axis.text.x = element_text(angle = 22.5, hjust=1))+\n  labs(fill = \"Percent Employted Type\") +\n  ylab(\"Median Percent Employed\") +\n  xlab(\"County Type\")+\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\n\n\n\n\nThe plot above illustrates median percentage employed in each job sector for each different county type, put on a percentage scale. One of the first relationships to observe is that employment in the mining sector is typically very low across all the county types, the exception as expected is the Mining-Dependant counties. This same pattern occurs for the Agriculture job sector and the Farm-Dependent counties, and it also occurs with Manufacturing sector and Manufacturing-Dependant counties. The Trans, Trade, Govt, Information, FIRE, and Construction job sectors all appear to have a very similar employment rates across all the county types. The Services job sector takes a large portion of all employment in every county with some variance.\n\ndatatidy %&gt;% na.omit() %&gt;% ggplot( aes(x=County_type,y=PerCapitaInc,fill=County_type) ) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  geom_violin(alpha=.5,adjust=.6) +\n  geom_boxplot(width=.3,outlier.alpha = .05) +\n  theme(axis.text.x = element_text(angle = 30, hjust=1))+\n  theme(legend.position = \"none\")+ \n  scale_y_continuous(breaks=seq(0,80000,10000)) +\n  xlab(\"County Type\") +\n  ylab(\"Per Capita Income\") +\n  labs(title=\"Per Capita Income by County Type\")\n\n\n\n\n\n\n\n\nThe plot above depicts the density of Counties’ Per Capita Income overlayed with a boxplot, separated by the county type. Each county type has outliers much larger than the mean, but it is most prevalent in the Federal/State Government, Nonspecialized, and Recreation County types. Both the Mining and Farm Dependent county types have larger densities around their means, compared to other groups where there is a more evenly distribution across the IQR."
  },
  {
    "objectID": "projects/ExploratoryDataAnalysis.html#pca-dimensionality-reduction",
    "href": "projects/ExploratoryDataAnalysis.html#pca-dimensionality-reduction",
    "title": "Exploratory Data Analysis",
    "section": "PCA Dimensionality Reduction:",
    "text": "PCA Dimensionality Reduction:\n\nData Preparation\n\npca_dat &lt;- data %&gt;% \n  na.omit() %&gt;% \n  select(-(1:5) ) %&gt;% \n  mutate( PerCapitaInc=(rank(PerCapitaInc)/length(PerCapitaInc))*100 )\n\nAll the selected variables are percentages, besides Per Capita Income. To put them on the same scale, Per Capita Income was given percentiles over the range 0 to 100.\n\n\nPerform PCA\n\ndata_pca &lt;- \n  pca_dat %&gt;% \n  princomp() \n\nsummary(data_pca)\n\nImportance of components:\n                           Comp.1     Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     29.2702350 8.56424651 7.68985909 4.99040321 4.32662727\nProportion of Variance  0.8018153 0.06864363 0.05534249 0.02330737 0.01751947\nCumulative Proportion   0.8018153 0.87045892 0.92580140 0.94910877 0.96662824\n                            Comp.6      Comp.7      Comp.8      Comp.9\nStandard deviation     3.121456421 3.021739991 2.397746391 2.012677740\nProportion of Variance 0.009118774 0.008545473 0.005380571 0.003791145\nCumulative Proportion  0.975747011 0.984292484 0.989673055 0.993464200\n                           Comp.10     Comp.11     Comp.12      Comp.13\nStandard deviation     1.693410754 1.343684508 1.311496455 0.7683802310\nProportion of Variance 0.002683778 0.001689727 0.001609742 0.0005525534\nCumulative Proportion  0.996147978 0.997837705 0.999447447 1.0000000000\n                            Comp.14\nStandard deviation     1.604685e-07\nProportion of Variance 2.409914e-17\nCumulative Proportion  1.000000e+00\n\ndata_pca$loadings[0:13,0:6]\n\n                           Comp.1      Comp.2        Comp.3        Comp.4\nPctEmpAgriculture    0.0084901049  0.41057340  0.5894542455  0.1694774821\nPctEmpMining         0.0078914988  0.01657434  0.1517254324 -0.0156807576\nPctEmpConstruction  -0.0003991241  0.02087666  0.0400933535 -0.0728817449\nPctEmpManufacturing  0.0445770916  0.42360350 -0.7680925585  0.1077846314\nPctEmpTrade          0.0012735157 -0.01910355 -0.0594706007 -0.0445961630\nPctEmpTrans          0.0081617573  0.04464634  0.0317068897 -0.0001245995\nPctEmpInformation   -0.0088145759 -0.01627969 -0.0011945891 -0.0095683714\nPctEmpFIRE          -0.0311941444 -0.05147966 -0.0208963852 -0.0141417086\nPctEmpServices      -0.0510225697 -0.75682140 -0.1040274222 -0.1590286844\nPctEmpGovt           0.0210364458 -0.07258994  0.1407016346  0.0387599159\nUnemploymentRate     0.0308530498 -0.06082507 -0.0207126785  0.0817257961\nPerCapitaInc        -0.9858236691  0.02397327 -0.0244387177  0.1562059416\nPerDeepPov           0.0715759748 -0.15342295  0.0008238906  0.3760549834\n                         Comp.5       Comp.6\nPctEmpAgriculture    0.56372441  0.108194110\nPctEmpMining        -0.54496989  0.502445234\nPctEmpConstruction  -0.17916413 -0.080287533\nPctEmpManufacturing  0.22838907 -0.002188520\nPctEmpTrade         -0.12372519  0.192630338\nPctEmpTrans         -0.18412846 -0.020448124\nPctEmpInformation    0.00743952 -0.008152730\nPctEmpFIRE          -0.02103559 -0.001692149\nPctEmpServices       0.44794824  0.123708173\nPctEmpGovt          -0.19447797 -0.814208801\nUnemploymentRate    -0.06038535 -0.090800922\nPerCapitaInc        -0.02753944 -0.012631883\nPerDeepPov           0.03586557  0.063416993\n\nPC_data &lt;- data %&gt;% \n  na.omit() %&gt;% \n  mutate(PC1=data_pca$scores[, 1],\n         PC2=data_pca$scores[, 2],\n         PC3=data_pca$scores[, 3],\n         PC4=data_pca$scores[, 4])\n\n\n\nScree Plot\n\neigval&lt;-data_pca$sdev^2\nvarprop=round(eigval/sum(eigval), 2) \n\nggplot() + \n  geom_line(aes(y=varprop, x=1:14), stat=\"identity\") + \n  xlab(\"\") + \n  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) +   \n  scale_x_continuous(breaks=1:10)\n\n\n\n\n\n\n\n\nBased ont the PCA’s summary, the first 2 Principal Components together explain 80% of the variance. Also illustrated above, the Scree Plot levels off after the second principal component. But, for exploratory purposes PC3 and PC4 will be included in the rest of the analysis.\n\n\nPC1 and PC2\n\nLoading Plots for PC1 and PC2\n\ndata_pca$loadings[1:14, 1:2] %&gt;% as.data.frame %&gt;% rownames_to_column %&gt;% ggplot() + \n  geom_hline(aes(yintercept=0), lty=5) +   \n  geom_vline(aes(xintercept=0), lty=5) + \n  ylab(\"PC2\") + \n  xlab(\"PC1\") +   \n  geom_segment(aes(x=0, y=0, xend=Comp.1, yend=Comp.2), arrow=arrow(), col=\"red\") +   \n  geom_label(aes(x=Comp.1*.8, y=Comp.2*.8, label=rowname), size = 3)\n\n\n\n\n\n\n\n\nThe loading plot illustrates the the contribution of each Variable to each Principal Component. The Per Capita Income variable is the main variable associated with PC1. Percent employed in Agriculture, Services, and Manufacturing were the major contributors for PC2.\n\n\nScore Plots for PC1 and PC2\n\nggplot(PC_data, aes(PC1, PC2)) + \n  geom_point( aes(color=County_type) ) +\n  facet_wrap(~County_type)\n\n\n\n\n\n\n\n\nEach point represents a singular FIPS code in the score plot above, but it was colored and separated by the County Type (which was not included in the PCA) to find if the PCA was able to discriminate between the different types. It was unable to perform a clear differentiation between categories, as there is no distinct grouping as there is much overlap between County Types with very similar cluster centers.\n\nggplot(PC_data, aes(PC1, PC2)) + \n  aes(color=PctEmpServices ,size=PerCapitaInc) +\n  geom_point( alpha= 0.4) +\n  scale_color_continuous(low=\"blue\",high=\"red\") +\n  facet_wrap(~County_type)\n\n\n\n\n\n\n\n\nThe above score plot above adds Percent employed in the sevices sector (a main contributing factor in PC2) mapped to color, and therefore the variance explained by tis variable is illustrated along the PC2 axis. This depiction was also applied to PC1 for the Per Capita Income mapped to size. This plot is just a visual representation of the singular major factors of variation along PC1 and PC2.\n\n\n\nPC3 and PC4\n\nLoading Plots for PC3 and PC4\n\ndata_pca$loadings[1:14, 3:4] %&gt;% as.data.frame %&gt;% rownames_to_column %&gt;% ggplot() + \n  geom_hline(aes(yintercept=0), lty=5) +   \n  geom_vline(aes(xintercept=0), lty=5) + \n  ylab(\"PC4\") + \n  xlab(\"PC3\") +   \n  geom_segment(aes(x=0, y=0, xend=Comp.3, yend=Comp.4), arrow=arrow(), col=\"red\") +   \n  geom_label(aes(x=Comp.3*.6, y=Comp.4*.85, label=rowname), size = 3)\n\n\n\n\n\n\n\n\nThis second loading plot illustrates the the next two Principal Components. The Pecerent Employed in Manufacturing and Agriculture are the main variables associated with PC3. Percent of Children in Deep Poverty and Percent of people in Deep Poverty were the major contributors for PC4.\n\n\nScore Plots for PC3 and PC4\n\nggplot(PC_data, aes(PC3, PC4)) + \n  geom_point(aes(color=PctEmpManufacturing,size=PctEmpAgriculture))  +\n  scale_color_continuous(low=\"blue\",high=\"red\")\n\n\n\n\n\n\n\n\nThe score plot above has Percent employed in the Manufacturing sector (a main contributing factor in PC3) mapped to color, and therefore the varience explained by this variable is illustrated along the PC3 axis. This depiction was also applied for the other coontributing variable for PC3: the Percent employed in the Agriculture sector. This was mapped to size to be a visual representation of how the loading values apply to the PCA score plot."
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html",
    "href": "projects/ModelingTestingPredicting.html",
    "title": "Modeling, Testing, and Predicting",
    "section": "",
    "text": "With the ongoing pandemic I went to HealthData.gov and found their Community Profile Report (CPR) – County-Level. The following is some information on this dataset based on their documentation. It was developed by Data Strategy and Execution Workgroup in the Joint Coordination Cell, under the White House COVID-19 Team. Each observation in the dataset is county-level. It contains daily snapshots in time that focuses on recent COVID-19 outcomes in the last seven days and changes relative to the week prior."
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#introduction",
    "href": "projects/ModelingTestingPredicting.html#introduction",
    "title": "Modeling, Testing, and Predicting",
    "section": "",
    "text": "With the ongoing pandemic I went to HealthData.gov and found their Community Profile Report (CPR) – County-Level. The following is some information on this dataset based on their documentation. It was developed by Data Strategy and Execution Workgroup in the Joint Coordination Cell, under the White House COVID-19 Team. Each observation in the dataset is county-level. It contains daily snapshots in time that focuses on recent COVID-19 outcomes in the last seven days and changes relative to the week prior."
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#package-and-data-import",
    "href": "projects/ModelingTestingPredicting.html#package-and-data-import",
    "title": "Modeling, Testing, and Predicting",
    "section": "Package and Data import",
    "text": "Package and Data import\n\nPackages\n\nlibrary(tidyverse)\nlibrary(mvtnorm)\nlibrary(ggExtra)\nlibrary(rstatix)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(interactions)\nlibrary(plotROC)\nlibrary(knitr)\n\n\n\nData Import\nHere I defined high and low cases by above and below the median value, and recoded the states into regions (NE:North East,SE:South East,MW:Midwest,SW:South West,W:West).\n\nFullCovid &lt;- read_csv(\"data/ModelingTestingPredicting/COVID-19_Community_Profile_Report_-_County-Level.csv\") \nCovidData &lt;- FullCovid %&gt;% \n  mutate( highCases = total_cases&gt;median(total_cases,na.rm=T) ) %&gt;% \n  mutate( region=recode(state, ME=\"NE\", MA=\"NE\", RI=\"NE\", CT=\"NE\", NH=\"NE\", VT=\"NE\", NY=\"NE\", PA=\"NE\", NJ=\"NE\", DE=\"NE\", MD=\"NE\", DC=\"NE\", PR=\"SE\", WV=\"SE\", VA=\"SE\", KY=\"SE\", TN=\"SE\", NC=\"SE\", SC=\"SE\", GA=\"SE\", AL=\"SE\", MS=\"SE\", AR=\"SE\", LA=\"SE\", FL=\"SE\", OH=\"MW\", IN=\"MW\", MI=\"MW\", IL=\"MW\", MO=\"MW\", WI=\"MW\", MN=\"MW\", IA=\"MW\", KS=\"MW\", NE=\"MW\", SD=\"MW\", ND=\"MW\", TX=\"SW\", OK=\"SW\", NM=\"SW\", AZ=\"SW\", CO=\"W\", WY=\"W\", MT=\"W\", ID=\"W\", WA=\"W\", OR=\"W\", UT=\"W\", NV=\"W\", CA=\"W\", AK=\"W\", HI=\"W\") ) %&gt;% \n  select(fips, county, region, highCases, everything(), -state,-fema_region)\n\n\nkable(head(CovidData))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfips\ncounty\nregion\nhighCases\ndate\ncases_last_7_days\ncases_per_100k_last_7_days\ntotal_cases\ncases_pct_change_from_prev_week\ndeaths_last_7_days\ndeaths_per_100k_last_7_days\ntotal_deaths\ndeaths_pct_change_from_prev_week\ntest_positivity_rate_last_7_days\ntotal_positive_tests_last_7_days\ntotal_tests_last_7_days\ntotal_tests_per_100k_last_7_days\ntest_positivity_rate_pct_change_from_prev_week\ntotal_tests_pct_change_from_prev_week\nconfirmed_covid_hosp_last_7_days\nconfirmed_covid_hosp_per_100_beds_last_7_days\nconfirmed_covid_hosp_per_100_beds_pct_change_from_prev_week\nsuspected_covid_hosp_last_7_days\nsuspected_covid_hosp_per_100_beds_last_7_days\nsuspected_covid_hosp_per_100_beds_pct_change_from_prev_week\npct_inpatient_beds_used_avg_last_7_days\npct_inpatient_beds_used_abs_change_from_prev_week\npct_inpatient_beds_used_covid_avg_last_7_days\npct_inpatient_beds_used_covid_abs_change_from_prev_week\npct_icu_beds_used_avg_last_7_days\npct_icu_beds_used_abs_change_from_prev_week\npct_icu_beds_used_covid_avg_last_7_days\npct_icu_beds_used_covid_abs_change_from_prev_week\npct_vents_used_avg_last_7_days\npct_vents_used_abs_change_from_prev_week\npct_vents_used_covid_avg_last_7_days\npct_vents_used_covid_abs_change_from_prev_week\n\n\n\n\n12086\nMiami-Dade County, FL\nSE\nTRUE\n04/26/2021 12:00:00 AM\n7831\n17645.807\n479426\n-0.258\n76\n225.437\n6125\n0.118\n0.109\n13165\n134217\n4940.006\n-0.003\n-0.076\n543\n9.005\n-0.053\n161\n2.670\n-0.094\n0.846\n0.009\n0.094\n0.002\n0.793\n-0.007\n0.146\n0.004\n0.276\n0.001\n0.050\n-0.005\n\n\n17031\nCook County, IL\nMW\nTRUE\n04/26/2021 12:00:00 AM\n7451\n10290.855\n530003\n-0.140\n65\n192.224\n9900\n0.016\n0.045\n7114\n164748\n3198.846\n-0.007\n-0.039\n714\n5.034\n-0.191\n1093\n7.706\n-0.111\n0.731\n-0.002\n0.069\n0.002\n0.680\n-0.005\n0.130\n0.000\n0.274\n0.005\n0.040\n0.004\n\n\n26163\nWayne County, MI\nMW\nTRUE\n04/26/2021 12:00:00 AM\n6780\n8671.941\n151702\n-0.299\n104\n259.869\n4546\n0.209\n0.144\n7393\n55022\n3145.295\n-0.022\n-0.021\n738\n17.141\n-0.170\n158\n3.670\n-0.091\n0.867\n-0.012\n0.204\n-0.013\n0.849\n0.023\n0.362\n0.024\n0.411\n0.044\n0.157\n0.032\n\n\n36047\nKings County, NY\nNE\nTRUE\n04/26/2021 12:00:00 AM\n4640\n10560.908\n270349\n-0.271\n91\n394.273\n10093\n-0.242\n0.041\n5448\n144694\n5652.324\n-0.008\n-0.085\n321\n7.341\n-0.122\n102\n2.333\n-0.406\n0.709\n-0.002\n0.106\n-0.016\n0.773\n-0.017\n0.199\n-0.022\n0.223\n-0.008\n0.035\n-0.008\n\n\n12011\nBroward County, FL\nSE\nTRUE\n04/26/2021 12:00:00 AM\n4551\n11963.674\n233624\n-0.204\n84\n146.868\n2868\n0.333\n0.109\n6415\n66844\n3423.021\n-0.004\n-0.014\n491\n10.466\n-0.066\n103\n2.196\n0.056\n0.802\n0.017\n0.112\n0.003\n0.849\n0.001\n0.215\n0.004\n0.371\n-0.006\n0.093\n0.003\n\n\n26125\nOakland County, MI\nMW\nTRUE\n04/26/2021 12:00:00 AM\n4214\n8656.122\n108858\n-0.336\n42\n172.951\n2175\n0.135\n0.107\n3850\n36464\n2899.528\n-0.029\n0.023\n600\n16.255\n-0.214\n106\n2.872\n-0.200\n0.814\n-0.032\n0.186\n-0.017\n0.824\n0.007\n0.277\n0.008\n0.287\n0.010\n0.087\n0.002"
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#manova",
    "href": "projects/ModelingTestingPredicting.html#manova",
    "title": "Modeling, Testing, and Predicting",
    "section": "MANOVA",
    "text": "MANOVA\n\nRun MANOVA\nThe MANOVA results indicate that total cases, total deaths, cases from the last 7 days, deaths from the last 7 days, test positivity rate from the last 7 days, and confirmed COVID hospitalizations show a mean difference across different regions of the US.\n\nman1 &lt;- manova(\n  cbind(\n    total_cases,\n    total_deaths, \n    cases_last_7_days, \n    deaths_last_7_days, \n    test_positivity_rate_last_7_days,\n    confirmed_covid_hosp_last_7_days\n  )  ~ \n  region, \n  data=CovidData\n)\nsummary(man1)\n\n            Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nregion       4 0.15215   15.882     24   9640 &lt; 2.2e-16 ***\nResiduals 2412                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nUnivariate ANOVAs\nPost-hoc univariate ANOVAs each show the prior mentioned predictor variables illustrate a mean difference across region.\n\nsummary.aov(man1)\n\n Response total_cases :\n              Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    \nregion         4 7.0150e+10 1.7538e+10  19.705 5.634e-16 ***\nResiduals   2412 2.1467e+12 8.8999e+08                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response total_deaths :\n              Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \nregion         4  52634377 13158594  39.151 &lt; 2.2e-16 ***\nResiduals   2412 810675863   336101                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response cases_last_7_days :\n              Df    Sum Sq Mean Sq F value    Pr(&gt;F)    \nregion         4  24209477 6052369  30.236 &lt; 2.2e-16 ***\nResiduals   2412 482815519  200172                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response deaths_last_7_days :\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nregion         4   3715  928.81  26.256 &lt; 2.2e-16 ***\nResiduals   2412  85324   35.37                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response test_positivity_rate_last_7_days :\n              Df Sum Sq   Mean Sq F value    Pr(&gt;F)    \nregion         4 0.0651 0.0162849  8.5578 7.436e-07 ***\nResiduals   2412 4.5899 0.0019029                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response confirmed_covid_hosp_last_7_days :\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nregion         4  195759   48940  23.928 &lt; 2.2e-16 ***\nResiduals   2412 4933260    2045                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n855 observations deleted due to missingness\n\n\n\n\nPost-hoc t-tests\nThe below matrices illustrate which group means significantly differ by region and by predictor variable (accounting for the change in significance level mentioned below)\n\n#Total Cases\n# Total Cases\ntotal_cases &lt;- \n  pairwise.t.test(\n    CovidData$total_cases, \n    CovidData$region, \n    p.adj = \"none\"\n  )$p.value * 67 &gt; 0.05\nkable(total_cases)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nFALSE\nNA\nNA\nNA\n\n\nSE\nTRUE\nFALSE\nNA\nNA\n\n\nSW\nTRUE\nFALSE\nTRUE\nNA\n\n\nW\nFALSE\nTRUE\nFALSE\nTRUE\n\n\n\n\n#Total Deaths\ntotal_deaths &lt;- \n  pairwise.t.test(\n    CovidData$total_deaths, \n    CovidData$region, \n    p.adj=\"none\"\n  )$p.value *67 &gt; 0.05\nkable(total_deaths)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nFALSE\nNA\nNA\nNA\n\n\nSE\nTRUE\nFALSE\nNA\nNA\n\n\nSW\nTRUE\nFALSE\nTRUE\nNA\n\n\nW\nTRUE\nFALSE\nTRUE\nTRUE\n\n\n\n\n#Cases in the Last 7 Days\ncases_last_7_days &lt;- \n  pairwise.t.test(\n    CovidData$cases_last_7_days, \n    CovidData$region, \n    p.adj=\"none\"\n  )$p.value * 67 &gt; 0.05\nkable(cases_last_7_days)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nFALSE\nNA\nNA\nNA\n\n\nSE\nTRUE\nFALSE\nNA\nNA\n\n\nSW\nTRUE\nFALSE\nTRUE\nNA\n\n\nW\nTRUE\nFALSE\nTRUE\nTRUE\n\n\n\n\n#Deaths in the Last 7 Days\ndeaths_last_7_days &lt;- \n  pairwise.t.test(\n    CovidData$deaths_last_7_days, \n    CovidData$region, \n    p.adj=\"none\"\n  )$p.value * 67 &gt; 0.05\nkable(deaths_last_7_days)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nFALSE\nNA\nNA\nNA\n\n\nSE\nTRUE\nFALSE\nNA\nNA\n\n\nSW\nTRUE\nFALSE\nTRUE\nNA\n\n\nW\nTRUE\nFALSE\nTRUE\nTRUE\n\n\n\n\n#Test Positivity Rate in the Last 7 Days\ntest_positivity_rate_last_7_days &lt;- \n  pairwise.t.test(\n    CovidData$test_positivity_rate_last_7_days, \n    CovidData$region, \n    p.adj=\"none\"\n  )$p.value * 67 &gt; 0.05\nkable(test_positivity_rate_last_7_days)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nTRUE\nNA\nNA\nNA\n\n\nSE\nFALSE\nFALSE\nNA\nNA\n\n\nSW\nFALSE\nTRUE\nFALSE\nNA\n\n\nW\nTRUE\nTRUE\nFALSE\nTRUE\n\n\n\n\n#Confirmed Covid Hospitalization in the last 7 Days\nconfirmed_covid_hosp_last_7_days &lt;- \n  pairwise.t.test(\n    CovidData$confirmed_covid_hosp_last_7_days, \n    CovidData$region, \n    p.adj=\"none\"\n  )$p.value * 67 &gt; 0.05\nkable(confirmed_covid_hosp_last_7_days)\n\n\n\n\n\nMW\nNE\nSE\nSW\n\n\n\n\nNE\nFALSE\nNA\nNA\nNA\n\n\nSE\nTRUE\nFALSE\nNA\nNA\n\n\nSW\nTRUE\nFALSE\nTRUE\nNA\n\n\nW\nTRUE\nFALSE\nTRUE\nTRUE\n\n\n\n\n\n\nSignificance level:\nWith 6 numeric predictors and 5 catagorical groups, 67 inference tests were done. This creates a 96.8% chance of having a Type-1 Error. Thus a Bonferroni correction will be done to reduce the 0.05 signifcance level of α to 0.00075 (7.5e-4).*\n\n1-(0.95)^67 # type one error rate\n\n[1] 0.9678277\n\n0.05/67 # bonferroni correction\n\n[1] 0.0007462687\n\n\n\n\n\nMANOVA assumptions\nThe MANOVA fails the first assumption of normality, there are many other assumptions to check that are harder to meet such as Homogeneity of within-group covariance, linear relationships among dependent variables, and the absence of outliers. This MANOVA is just a proof of concept example.\n\ngroup &lt;- CovidData %&gt;% \n  na.omit() %&gt;% \n  select(region) %&gt;% \n  mutate(region = as.factor(region))\n\nDVs &lt;- \n  CovidData %&gt;% \n  na.omit() %&gt;% \n  select(\n    total_cases,\n    total_deaths, \n    cases_last_7_days, \n    deaths_last_7_days, \n    test_positivity_rate_last_7_days, \n    confirmed_covid_hosp_last_7_days\n  ) \n\n#Test multivariate normality for each group (null: normality met)\nkable(sapply(split(DVs,group), mshapiro_test))\n\n\n\n\n\n\n\n\n\n\n\n\n\nMW\nNE\nSE\nSW\nW\n\n\n\n\nstatistic\nc(W = 0.34489267113313)\nc(W = 0.607959552000503)\nc(W = 0.43185761076474)\nc(W = 0.350905905314517)\nc(W = 0.254613320396152)\n\n\np.value\n2.09014628046061e-26\n1.31446590550276e-17\n1.09889494210124e-28\n1.1706250436637e-16\n3.08035475306469e-16"
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#randomization-test",
    "href": "projects/ModelingTestingPredicting.html#randomization-test",
    "title": "Modeling, Testing, and Predicting",
    "section": "Randomization test",
    "text": "Randomization test\nThe Null Hypothesis of this Randomization Test is that there is no correlation between positive test rate in the past 7 days and the number of cases the past 7 days. The null distribution of the correlation coefficient is illustrated in dark grey, and the in-sample correlation coefficient is the red vertical line. The probability of a value as extreme as the in-sample value under this “randomization distribution” is 0, therefore we reject the null hypothesis that there is no correlation between these 2 variables.\n\nrandData &lt;- \n  CovidData %&gt;% \n  na.omit() %&gt;% \n  select(\n    test_positivity_rate_last_7_days,\n    cases_last_7_days\n  )\n\n\n\n# Calcualte possible correlations \n# based on collected data shuffling \n# to the variables being correlated\nrand_dist&lt;-vector()\nfor(i in 1:10000){\n  new &lt;- data.frame(\n    # randomly sample without replacement\n    positives=sample(\n      randData$test_positivity_rate_last_7_days\n    ),\n    cases=randData$cases_last_7_days) \n  \n  # find correlation for the shuffled data\n  rand_dist[i]&lt;- cor(new$positives, new$cases)\n}\n\nHere we calculate the p-value for the observed distribution compared to the null distribution.\n\n# Find the observed correlation\nsampleCor &lt;- \n  cor(\n    randData$test_positivity_rate_last_7_days, \n    randData$cases_last_7_days\n  )\n\npval &lt;- mean(rand_dist&gt;sampleCor | rand_dist &lt; -sampleCor)\npval\n\n[1] 0\n\n\n\nggplot( \n  data.frame(rand_dist), \n  aes(x=rand_dist) \n) + \n  geom_histogram(bins = 40) +\n  geom_vline(aes(xintercept=sampleCor), color=\"red\") +\n  theme_minimal() +\n  labs(\n    x = \"Randomized Values\",\n    y = \"Number of Itererations\"\n  )"
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#linear-regression-model",
    "href": "projects/ModelingTestingPredicting.html#linear-regression-model",
    "title": "Modeling, Testing, and Predicting",
    "section": "Linear Regression Model:",
    "text": "Linear Regression Model:\n\nCreating and Interpreting\nGiven average test positivity rate over the last 7 days and average confirmed COVID hospitalizations over the last 7 days the predicted value of cases the last 7 days is 152.968 cases.\n431.0901 is the slope for test positivity rate over the last 7 days on cases the last 7 days while holding confirmed covid hospitalizations over the last 7 days constant\n8.5943 is the slope for confirmed covid hospitalizations over the last 7 days on cases the last 7 days while holding test positivity rate over the last 7 days constant\nThe effect of positivity rate over the last 7 days is 11.9381 cases higher for every percent above the mean positivity rate over the last 7 days is.\nThis linear model explains 82.42% of variance in cases the last 7 days\n\nlmData &lt;- CovidData %&gt;% \n  select(\n    cases_last_7_days, \n    test_positivity_rate_last_7_days, \n    confirmed_covid_hosp_last_7_days ) %&gt;% \n  # mean-center predictors\n  mutate( \n    posRate = \n      test_positivity_rate_last_7_days-\n        mean(test_positivity_rate_last_7_days,na.rm=T),\n    confirmHosp = \n      confirmed_covid_hosp_last_7_days-\n        mean(CovidData$confirmed_covid_hosp_last_7_days,na.rm=T)\n    )\n\nfit&lt;-lm(cases_last_7_days~posRate*confirmHosp, data=lmData)\nsummary(fit)\n\n\nCall:\nlm(formula = cases_last_7_days ~ posRate * confirmHosp, data = lmData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2039.63   -33.24   -14.33     1.47  2708.79 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         152.9680     3.9324  38.899  &lt; 2e-16 ***\nposRate             431.0901    90.5095   4.763 2.02e-06 ***\nconfirmHosp           8.6846     0.1005  86.457  &lt; 2e-16 ***\nposRate:confirmHosp  11.9381     2.1726   5.495 4.32e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 192.2 on 2413 degrees of freedom\n  (855 observations deleted due to missingness)\nMultiple R-squared:  0.8242,    Adjusted R-squared:  0.824 \nF-statistic:  3771 on 3 and 2413 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPlot of the Interaction\n\ninteract_plot(fit, confirmHosp, posRate, plot.points = T)\n\n\n\n\n\n\n\n\n\n\nLinear Model Assumptions\nThis linear model fails normality and homoscedasticity and is only to be used an example of the method and interpretation due to these failure of assumptions.\n\n# Normality\nshapiro.test(fit$residuals) #H0: true distribution is normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  fit$residuals\nW = 0.49026, p-value &lt; 2.2e-16\n\n# Homoscedasticity\nggplot() + \n  aes(x=fit$fitted.values,y=fit$residuals) +\n  geom_point() +\n  theme_minimal() + \n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  )\n\n\n\n\n\n\n\nbptest(fit) # H0: homoskedastic\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit\nBP = 979.38, df = 3, p-value &lt; 2.2e-16\n\n\n\n\nRobust Standard Error (SE)\nBecause the model failed homoskeydacity so robust SE were used allowing for non-constant variance. The robust SE changed the significance of the interaction, increased the positivity rate over the last 7 days p-value, but not above 0.05, and decreased all t values.\n\ncoeftest(fit, vcov=vcovHC(fit))\n\n\nt test of coefficients:\n\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         152.96796    5.23374 29.2273  &lt; 2e-16 ***\nposRate             431.09008  170.74945  2.5247  0.01164 *  \nconfirmHosp           8.68462    0.69314 12.5295  &lt; 2e-16 ***\nposRate:confirmHosp  11.93807   14.54385  0.8208  0.41182    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBootstrapping SE\nThe bootstrapped SEs are very similar to robust SE (above) and much higher than the original SE, illustrating the robust SE are a more accurate measure of the sample than than the original SE.\n\nsamp_distn&lt;-replicate(5000, {  \n  #take bootstrap sample of rows  \n  boot_dat &lt;- sample_frac(lmData, replace=T) \n  #fit model on bootstrap sample  \n  bootfit &lt;- lm(cases_last_7_days~posRate*confirmHosp, data=boot_dat) \n  #save coefs\n  coef(bootfit) \n}) \n\nsamp_distn %&gt;% t() %&gt;% as.data.frame() %&gt;% summarize_all(sd)\n\n  (Intercept)  posRate confirmHosp posRate:confirmHosp\n1    5.052734 168.3468   0.6375838            14.74948"
  },
  {
    "objectID": "projects/ModelingTestingPredicting.html#logistic-regression-models",
    "href": "projects/ModelingTestingPredicting.html#logistic-regression-models",
    "title": "Modeling, Testing, and Predicting",
    "section": "Logistic Regression Models:",
    "text": "Logistic Regression Models:\n\nLogistic Model with 2 predictors\n\nCreating and Interpreting\nGiven the model’s coefficients: for every 1 unit increase in cases in the last 7 days the odds of highCases incrase by 1.04, and for every 1 unit increase in total deaths the odds of highCases increase by 1.06\n\nfit2 &lt;- glm(\n  highCases~cases_last_7_days+total_deaths,\n  data=CovidData, \n  family=\"binomial\")\nsummary(fit2)\n\n\nexp(fit2$coefficients)\n\n      (Intercept) cases_last_7_days      total_deaths \n       0.02213204        1.03672930        1.05724118 \n\n\n\n\nConfusion Matrix\nFrom the confusion matrix it appears the model is doing a decent job at classifying points with as little as 5.99% to 12.23% error.\n\nCovidData &lt;- CovidData %&gt;% \n  filter( complete.cases(CovidData$highCases),\n          complete.cases(CovidData$cases_last_7_days),\n          complete.cases(CovidData$total_deaths) )\n\ntable(Predicted = (predict(fit2,type=\"response\")&gt;0.5) , \n      Actual = CovidData$highCases) %&gt;% \n  as.data.frame() -&gt; conf_mat\n\nconf_mat &lt;- conf_mat %&gt;% \n  group_by(Actual) %&gt;% \n  mutate(percent=Freq/sum(Freq))\n\nggplot(conf_mat) +\n  aes(y=factor(Predicted,levels=c(F,T)),\n      x=factor(Actual,levels=c(T,F)),\n      fill=Freq,\n      label=paste0(round(percent*100,2),\"%\") ) +\n  geom_tile() +\n  ggfittext::geom_fit_text(contrast = TRUE, reflow =TRUE) +\n  scale_colour_discrete() +\n  coord_fixed() + \n  xlab(\"Actual\") + \n  ylab(\"Predicted\") +\n  theme_minimal()+ \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe below table and provides diagnostic measures of the predictive power of the Logistic Model with 2 predictors.\n\nclass_diag&lt;-function(probs,truth){\n  \n  tab&lt;-table(factor(probs&gt;.5,levels=c(\"FALSE\",\"TRUE\")),truth)\n  acc=sum(diag(tab))/sum(tab)\n  sens=tab[2,2]/colSums(tab)[2]\n  spec=tab[1,1]/colSums(tab)[1]\n  ppv=tab[2,2]/rowSums(tab)[2]\n\n  if( is.numeric(truth)==FALSE & \n      is.logical(truth)==FALSE){\n    truth&lt;-as.numeric(truth)-1\n  }\n  \n  #CALCULATE EXACT AUC\n  ord&lt;-order(probs, decreasing=TRUE)\n  probs &lt;- probs[ord]; truth &lt;- truth[ord]\n  \n  TPR=cumsum(truth)/max(1,sum(truth)) \n  FPR=cumsum(!truth)/max(1,sum(!truth))\n  \n  dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)\n  TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)\n  \n  n &lt;- length(TPR)\n  auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )\n\n  data.frame(\n    Accuracy = acc,\n    Sensitivity = sens,\n    Specificity = spec,\n    PPV = ppv, \n    AUC = auc)\n}\n\nprobs &lt;- predict(fit2)\nmetrics &lt;- class_diag(probs, CovidData$highCases)\n\noutput_2pred &lt;- \n  metrics %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rename(`Value(2)` = \"TRUE\") %&gt;%\n  mutate(`Value(2)` = paste0(round(`Value(2)`*100,2),\"%\"))\n\nkable(output_2pred)\n\n\n\n\n\nValue(2)\n\n\n\n\nAccuracy\n89.94%\n\n\nSensitivity\n83.49%\n\n\nSpecificity\n96.39%\n\n\nPPV\n95.86%\n\n\nAUC\n97.21%\n\n\n\n\n\n\n\nDensity Plot\nThis density plot illustrates the predictor value (logit) compared to the actual group membership to highCases.\n\ndensityVal &lt;- CovidData %&gt;% mutate(logit=fitted(fit2) )\n\nggplot(densityVal) + \n  aes(x=logit, fill=highCases) +\n  geom_density(alpha=.5) +\n  theme_minimal() + \n  labs(\n    x = \"logit\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\nROC and AUC\nThis ROC curve is of the logistic model with 2 predictors. Based on the shape of the ROC curve and the AUC of the ROC curve, the classification of this 2 predictor logistical model is very good\n\nROCplot &lt;- \n  CovidData %&gt;%\n  mutate(\n    highCases = \n      case_when(\n        highCases ~ 1,\n        !highCases ~ 0,\n        .default = NA\n      )\n  ) %&gt;%\n  ggplot()+\n  aes(d=highCases,m=cases_last_7_days+total_deaths) +\n  geom_roc(n.cuts=0)  +\n  theme_minimal()\n\nROCplot\n\n\n\n\n\n\n\ncalc_auc(ROCplot)\n\n  PANEL group       AUC\n1     1    -1 0.9704336\n\n\n\n\n\nLogistic Model with all predictors\n\nCreating and Interpreting\nFrom including all variables the model was made more flexible, but had lower values for Accuracy, Sensitivity, and AUC, as there was more noise introduced.\n\nlassoData &lt;- CovidData %&gt;% \n  select(-fips, -county,-date) %&gt;% \n  na.omit\nfit3 &lt;- lm(highCases~(.), data=lassoData)\nprobs &lt;- predict(fit3,data=lassoData)\nmetrics &lt;- class_diag(probs, lassoData$highCases)\n\noutput_all &lt;- \n  metrics %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rename(`Value(All)` = \"TRUE\") %&gt;%\n  mutate(`Value(All)` = paste0(round(`Value(All)`*100,2),\"%\"))\n\nkable(output_all %&gt;% cbind(output_2pred))\n\n\n\n\n\nValue(All)\nValue(2)\n\n\n\n\nAccuracy\n93.39%\n89.94%\n\n\nSensitivity\n100%\n83.49%\n\n\nSpecificity\n1.96%\n96.39%\n\n\nPPV\n93.38%\n95.86%\n\n\nAUC\n93.59%\n97.21%\n\n\n\n\n\n\n\n10-Fold Cross Validation (CV)\nThere is surprisingly no sign of overfitting the more flexible model by using all predictors. When having many degrees of freedom you can easily overfit the CV criteria for the model to be tuned to exploit the random variation in the dataset rather than improve predictive performance, which is what likely is occurring here. To counteract this I will use LASSO will to reduce the degrees of freedom (number of variables used in prediction).\n\nk=10\n\ncvData&lt;-lassoData[sample(nrow(lassoData)),] #randomly order rows\nfolds&lt;-cut(seq(1:nrow(lassoData)),breaks=k,labels=F) #create folds\n\ndiags&lt;-NULL\nfor(i in 1:k){\n  ## Create training and test sets  \n  train&lt;-cvData[folds!=i,]   \n  test&lt;-cvData[folds==i,]  \n  truth&lt;-test$highCases ## Truth labels for fold i\n  \n  ## Train model on training set (all but fold i)  \n  lassofit&lt;-glm(highCases~(.),data=train,family=\"binomial\")\n  \n  ## Test model on test set (fold i)   \n  probs&lt;-predict(lassofit,newdata = test,type=\"response\")\n  \n  ## Get diagnostics for fold i  \n  diags&lt;-rbind(diags,class_diag(probs,truth))\n}\n\n\n#average diagnostics across all k folds\nsummarize_all(diags,mean) %&gt;%\n  t() %&gt;%\n  kable()\n\n\n\n\nAccuracy\n0.9682807\n\n\nSensitivity\n0.9787905\n\n\nSpecificity\n0.7966667\n\n\nPPV\n0.9871751\n\n\nAUC\n0.9293887\n\n\n\n\n\n\n\nLASSO\nThe variables that were selected by LASSO were those that increased the ability of the model to predict highCases and did not add extra noise in the data for the model to learn. We can see that most of the variables were selected, so the CV critera was not overfit.\n\nlibrary(glmnet)\n\ny&lt;-as.matrix(lassoData$highCases) #grab response\nx&lt;-model.matrix(highCases~(.),data=lassoData)[,-1] #grab predictors\n\ncv&lt;-cv.glmnet(x,y,family=\"binomial\")\n\nWarning: from glmnet C++ code (error code -97); Convergence for 97th lambda\nvalue not reached after maxit=100000 iterations; solutions for larger lambdas\nreturned\n\nlasso&lt;-glmnet(x,y,family=\"binomial\",lambda=cv$lambda.1se)\n\nvar_to_select &lt;- dimnames(coef(lasso))[[1]][coef(lasso)@i +1]\n\n\nnewLassoData &lt;- lassoData %&gt;% \n  mutate(regionNE=(region==\"NE\"))  %&gt;% \n  mutate(regionSW=(region==\"SW\")) %&gt;% \n  mutate(regionW =(region==\"W\" ))\n\nlassoSelectedVarFunction &lt;- highCases~\n  regionNE+\n  regionSW+\n  regionW+\n  cases_per_100k_last_7_days+\n  total_cases+\n  cases_pct_change_from_prev_week+\n  deaths_per_100k_last_7_days+\n  total_deaths+\n  deaths_pct_change_from_prev_week+\n  test_positivity_rate_last_7_days+\n  total_tests_last_7_days+\n  total_tests_per_100k_last_7_days+\n  test_positivity_rate_pct_change_from_prev_week+\n  total_tests_pct_change_from_prev_week+\n  confirmed_covid_hosp_last_7_days+\n  confirmed_covid_hosp_per_100_beds_last_7_days+\n  confirmed_covid_hosp_per_100_beds_pct_change_from_prev_week+\n  suspected_covid_hosp_last_7_days+\n  suspected_covid_hosp_per_100_beds_last_7_days+\n  suspected_covid_hosp_per_100_beds_pct_change_from_prev_week+\n  pct_inpatient_beds_used_avg_last_7_days+\n  pct_inpatient_beds_used_abs_change_from_prev_week+\n  pct_icu_beds_used_avg_last_7_days+\n  pct_icu_beds_used_abs_change_from_prev_week+\n  pct_icu_beds_used_covid_avg_last_7_days+\n  pct_icu_beds_used_covid_abs_change_from_prev_week+\n  pct_vents_used_avg_last_7_days+\n  pct_vents_used_abs_change_from_prev_week+\n  pct_vents_used_covid_avg_last_7_days+\n  pct_vents_used_covid_abs_change_from_prev_week\n\nlassoSelectedVarFunction\n\nhighCases ~ regionNE + regionSW + regionW + cases_per_100k_last_7_days + \n    total_cases + cases_pct_change_from_prev_week + deaths_per_100k_last_7_days + \n    total_deaths + deaths_pct_change_from_prev_week + test_positivity_rate_last_7_days + \n    total_tests_last_7_days + total_tests_per_100k_last_7_days + \n    test_positivity_rate_pct_change_from_prev_week + total_tests_pct_change_from_prev_week + \n    confirmed_covid_hosp_last_7_days + confirmed_covid_hosp_per_100_beds_last_7_days + \n    confirmed_covid_hosp_per_100_beds_pct_change_from_prev_week + \n    suspected_covid_hosp_last_7_days + suspected_covid_hosp_per_100_beds_last_7_days + \n    suspected_covid_hosp_per_100_beds_pct_change_from_prev_week + \n    pct_inpatient_beds_used_avg_last_7_days + pct_inpatient_beds_used_abs_change_from_prev_week + \n    pct_icu_beds_used_avg_last_7_days + pct_icu_beds_used_abs_change_from_prev_week + \n    pct_icu_beds_used_covid_avg_last_7_days + pct_icu_beds_used_covid_abs_change_from_prev_week + \n    pct_vents_used_avg_last_7_days + pct_vents_used_abs_change_from_prev_week + \n    pct_vents_used_covid_avg_last_7_days + pct_vents_used_covid_abs_change_from_prev_week\n\n\n\n\nCV with LASSO Variables\n\n######## IN-SAMPLE ########\nfitLasso &lt;- glm(lassoSelectedVarFunction, data=newLassoData)\nprobs &lt;- predict(fitLasso)\n\noutput_lasso &lt;- \n  class_diag(probs, newLassoData$highCases) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rename(`Value(LASSO CV)` = \"TRUE\") %&gt;%\n  mutate(`Value(LASSO CV)` = paste0(round(`Value(LASSO CV)`*100,2),\"%\"))\n\n\n######## OUT-OF-SAMPLE ########\nk=10\ncvData&lt;-newLassoData[sample(nrow(newLassoData)),] #randomly order rows\nfolds&lt;-cut(seq(1:nrow(newLassoData)),breaks=k,labels=F) #create folds\n\ndiags&lt;-NULL\nfor(i in 1:k){\n  ## Create training and test sets  \n  train&lt;-cvData[folds!=i,]   \n  test&lt;-cvData[folds==i,]  \n  truth&lt;-test$highCases ## Truth labels for fold i\n\n  lassofit&lt;-glm(lassoSelectedVarFunction,data=train,family=\"binomial\")  ## Train model on training set (all but fold i)  \n  probs&lt;-predict(lassofit,newdata = test,type=\"response\") ## Test model on test set (fold i)   \n  diags&lt;-rbind(diags,class_diag(probs,truth))  ## Get diagnostics for fold i\n}\n\n\n#average diagnostics across all k folds\noutput_lasso_cv &lt;- \n  summarize_all(diags,mean) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rename(`Value(LASSO CV)` = \"V1\") %&gt;%\n  mutate(`Value(LASSO CV)` = paste0(round(`Value(LASSO CV)`*100,2),\"%\"))\n\nkable(\n  output_lasso_cv %&gt;% \n  cbind(output_lasso) %&gt;%\n  cbind(output_all) %&gt;% \n  cbind(output_2pred)\n)\n\n\n\n\n\nValue(LASSO CV)\nValue(LASSO CV)\nValue(All)\nValue(2)\n\n\n\n\nAccuracy\n97.89%\n93.39%\n93.39%\n89.94%\n\n\nSensitivity\n99.01%\n100%\n100%\n83.49%\n\n\nSpecificity\n81.12%\n1.96%\n1.96%\n96.39%\n\n\nPPV\n98.75%\n93.38%\n93.38%\n95.86%\n\n\nAUC\n94.21%\n92.9%\n93.59%\n97.21%\n\n\n\n\n\nUsing the variables that LASSO selected, the model’s out-of-sample AUC is higher than the model’s in-sample AUC. The out-of-sample AUC (CV) is higher than the in-sample AUC (without CV)."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my projects page. Below are some of the projects that I have made for fun to explore each topic. I hope you enjoy!\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to SQL with DuckDB\n\n\n\nduckdb\n\n\nSQL\n\n\n\nA beginner’s guide to SQL using DuckDB\n\n\n\nMaya\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to ML Classification\n\n\n\npytorch\n\n\nneural networks\n\n\n\nUsing the MNSIT dataset I train a MLP to classify handwritten numbers\n\n\n\nMaya\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModeling, Testing, and Predicting\n\n\n\n\n\n\nMaya Ylagan\n\n\nApr 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nMaya\n\n\nFeb 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/TTSBBC.html",
    "href": "publications/TTSBBC.html",
    "title": "TTSBBC",
    "section": "",
    "text": "Link to application\nAbout the application structure and resources - dbplyr - SQL - AWS - Docker/ECS"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website. I am builting my first one!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]