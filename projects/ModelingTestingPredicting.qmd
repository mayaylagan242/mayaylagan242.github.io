---
title: 'Modeling, Testing, and Predicting'
author: "Maya Ylagan"
date: '2021-04-20'
# image: "/images/projects/confusion_matrix-1.png"
description: ""
tags:
  - Markdown syntax
  - Logistic
  - Linear
  - ANOVA
  - Plotting
execute: 
  freeze: true
  cache: true
---

## Introduction

With the ongoing pandemic I went to HealthData.gov and found their Community Profile Report (CPR) – County-Level.  The following is some information on this dataset based on their documentation.  It was developed by Data Strategy and Execution Workgroup in the Joint Coordination Cell, under the White House COVID-19 Team. Each observation in the dataset is county-level.  It contains daily snapshots in time that focuses on recent COVID-19 outcomes in the last seven days and changes relative to the week prior.

```{r}
#| label: setup
#| include: FALSE

library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)

```



## Package and Data import 

#### Packages
```{r}
#| label: Packages
#| output: false

library(tidyverse)
library(mvtnorm)
library(ggExtra)
library(rstatix)
library(sandwich)
library(lmtest)
library(interactions)
library(plotROC)
library(knitr)

```


#### Data Import
Here I defined high and low cases by above and below the median value, and recoded the states into regions (NE:North East,SE:South East,MW:Midwest,SW:South West,W:West).
```{r}
#| label: Data
#| output: false


FullCovid <- read_csv("data/ModelingTestingPredicting/COVID-19_Community_Profile_Report_-_County-Level.csv") 
CovidData <- FullCovid %>% 
  mutate( highCases = total_cases>median(total_cases,na.rm=T) ) %>% 
  mutate( region=recode(state, ME="NE", MA="NE", RI="NE", CT="NE", NH="NE", VT="NE", NY="NE", PA="NE", NJ="NE", DE="NE", MD="NE", DC="NE", PR="SE", WV="SE", VA="SE", KY="SE", TN="SE", NC="SE", SC="SE", GA="SE", AL="SE", MS="SE", AR="SE", LA="SE", FL="SE", OH="MW", IN="MW", MI="MW", IL="MW", MO="MW", WI="MW", MN="MW", IA="MW", KS="MW", NE="MW", SD="MW", ND="MW", TX="SW", OK="SW", NM="SW", AZ="SW", CO="W", WY="W", MT="W", ID="W", WA="W", OR="W", UT="W", NV="W", CA="W", AK="W", HI="W") ) %>% 
  select(fips, county, region, highCases, everything(), -state,-fema_region)
```

```{r}
kable(head(CovidData))
```
  
---------------------------------------------------

## MANOVA
### Run MANOVA
The MANOVA results indicate that total cases, total deaths, cases from the last 7 days, deaths from the last 7 days, test positivity rate from the last 7 days, and confirmed COVID hospitalizations show a mean difference across different regions of the US.

```{r}
#| label: MANOVA

man1 <- manova(
  cbind(
    total_cases,
    total_deaths, 
    cases_last_7_days, 
    deaths_last_7_days, 
    test_positivity_rate_last_7_days,
    confirmed_covid_hosp_last_7_days
  )  ~ 
  region, 
  data=CovidData
)
summary(man1)
```


### Univariate ANOVAs
Post-hoc univariate ANOVAs each show the prior mentioned predictor variables illustrate a mean difference across region.
```{r}
#| label: Univariate_ANOVA

summary.aov(man1)
```

### Post-hoc t-tests
The below matrices illustrate which group means significantly differ by region and by predictor variable (accounting for the change in significance level mentioned below)
```{r}
#| label: posthoc_ttest

#Total Cases
# Total Cases
total_cases <- 
  pairwise.t.test(
    CovidData$total_cases, 
    CovidData$region, 
    p.adj = "none"
  )$p.value * 67 > 0.05
kable(total_cases)

#Total Deaths
total_deaths <- 
  pairwise.t.test(
    CovidData$total_deaths, 
    CovidData$region, 
    p.adj="none"
  )$p.value *67 > 0.05
kable(total_deaths)

#Cases in the Last 7 Days
cases_last_7_days <- 
  pairwise.t.test(
    CovidData$cases_last_7_days, 
    CovidData$region, 
    p.adj="none"
  )$p.value * 67 > 0.05
kable(cases_last_7_days)

#Deaths in the Last 7 Days
deaths_last_7_days <- 
  pairwise.t.test(
    CovidData$deaths_last_7_days, 
    CovidData$region, 
    p.adj="none"
  )$p.value * 67 > 0.05
kable(deaths_last_7_days)

#Test Positivity Rate in the Last 7 Days
test_positivity_rate_last_7_days <- 
  pairwise.t.test(
    CovidData$test_positivity_rate_last_7_days, 
    CovidData$region, 
    p.adj="none"
  )$p.value * 67 > 0.05
kable(test_positivity_rate_last_7_days)

#Confirmed Covid Hospitalization in the last 7 Days
confirmed_covid_hosp_last_7_days <- 
  pairwise.t.test(
    CovidData$confirmed_covid_hosp_last_7_days, 
    CovidData$region, 
    p.adj="none"
  )$p.value * 67 > 0.05
kable(confirmed_covid_hosp_last_7_days)
```


#### Significance level:
With 6 numeric predictors and 5 catagorical groups, 67 inference tests were done.  This creates a 96.8% chance of having a Type-1 Error.  Thus a Bonferroni correction will be done to reduce the 0.05 signifcance level of α to 0.00075 (7.5e-4).*
```{r}
#| label: alpha
1-(0.95)^67 # type one error rate
0.05/67 # bonferroni correction
```


### MANOVA assumptions
The MANOVA fails the first assumption of normality, there are  many other assumptions to check that are harder to meet such as Homogeneity of within-group covariance, linear relationships among dependent variables, and the absence of outliers.  This MANOVA is just a proof of concept example.
```{r}
#| label: MANOVA_assumptions

group <- CovidData %>% 
  na.omit() %>% 
  select(region) %>% 
  mutate(region = as.factor(region))

DVs <- 
  CovidData %>% 
  na.omit() %>% 
  select(
    total_cases,
    total_deaths, 
    cases_last_7_days, 
    deaths_last_7_days, 
    test_positivity_rate_last_7_days, 
    confirmed_covid_hosp_last_7_days
  ) 

#Test multivariate normality for each group (null: normality met)
kable(sapply(split(DVs,group), mshapiro_test))

```
  
  
  
-------------------------------


## Randomization test
The Null Hypothesis of this Randomization Test is that there is no correlation between `positive test rate in the past 7 days` and the `number of cases the past 7 days`.  The null distribution of the correlation coefficient is illustrated in dark grey, and the in-sample correlation coefficient is the red vertical line.  The probability of a value as extreme as the in-sample value under this "randomization distribution" is 0, therefore we reject the null hypothesis that there is no correlation between these 2 variables.
```{r}
#| label: randomization_test

randData <- 
  CovidData %>% 
  na.omit() %>% 
  select(
    test_positivity_rate_last_7_days,
    cases_last_7_days
  )



# Calcualte possible correlations 
# based on collected data shuffling 
# to the variables being correlated
rand_dist<-vector()
for(i in 1:10000){
  new <- data.frame(
    # randomly sample without replacement
    positives=sample(
      randData$test_positivity_rate_last_7_days
    ),
    cases=randData$cases_last_7_days) 
  
  # find correlation for the shuffled data
  rand_dist[i]<- cor(new$positives, new$cases)
}
```


Here we calculate the p-value for the observed distribution compared to the null distribution.
```{r}
#| label: randomization_pval


# Find the observed correlation
sampleCor <- 
  cor(
    randData$test_positivity_rate_last_7_days, 
    randData$cases_last_7_days
  )

pval <- mean(rand_dist>sampleCor | rand_dist < -sampleCor)
pval
```

```{r}
#| label: randomization_plot

ggplot( 
  data.frame(rand_dist), 
  aes(x=rand_dist) 
) + 
  geom_histogram(bins = 40) +
  geom_vline(aes(xintercept=sampleCor), color="red") +
  theme_minimal() +
  labs(
    x = "Randomized Values",
    y = "Number of Itererations"
  )
```

------------

## Linear Regression Model:

### Creating and Interpreting
Given average `test positivity rate over the last 7 days` and average `confirmed COVID hospitalizations over the last 7 days` the predicted value of `cases the last 7 days` is 152.968 cases.

431.0901 is the slope for `test positivity rate over the last 7 days` on `cases the last 7 days` while holding `confirmed covid hospitalizations over the last 7 days` constant

8.5943 is the slope for `confirmed covid hospitalizations over the last 7 days` on `cases the last 7 days` while holding `test positivity rate over the last 7 days` constant

The effect of `positivity rate over the last 7 days` is 11.9381 cases higher for every percent above the mean `positivity rate over the last 7 days` is.

This linear model explains 82.42% of variance in `cases the last 7 days`
```{r}
#| label: linear_model

lmData <- CovidData %>% 
  select(
    cases_last_7_days, 
    test_positivity_rate_last_7_days, 
    confirmed_covid_hosp_last_7_days ) %>% 
  # mean-center predictors
  mutate( 
    posRate = 
      test_positivity_rate_last_7_days-
        mean(test_positivity_rate_last_7_days,na.rm=T),
    confirmHosp = 
      confirmed_covid_hosp_last_7_days-
        mean(CovidData$confirmed_covid_hosp_last_7_days,na.rm=T)
    )

fit<-lm(cases_last_7_days~posRate*confirmHosp, data=lmData)
summary(fit)
```



### Plot of the Interaction 
```{r}
#| label: interaction_plot

interact_plot(fit, confirmHosp, posRate, plot.points = T)
```


### Linear Model Assumptions
This linear model fails normality and homoscedasticity and is only to be used an example of the method and interpretation due to these failure of assumptions.
```{r}
#| label: linear_model_assumptions

# Normality
shapiro.test(fit$residuals) #H0: true distribution is normal

# Homoscedasticity
ggplot() + 
  aes(x=fit$fitted.values,y=fit$residuals) +
  geom_point() +
  theme_minimal() + 
  labs(
    x = "Fitted Values",
    y = "Residuals"
  )
bptest(fit) # H0: homoskedastic

```


### Robust Standard Error (SE)
Because the model failed homoskeydacity so robust SE were used allowing for non-constant variance.  The robust SE changed the significance of the interaction, increased the `positivity rate over the last 7 days` p-value,  but not above 0.05, and decreased all t values.

```{r}
#| label: robust_SE

coeftest(fit, vcov=vcovHC(fit))
```


### Bootstrapping SE
The bootstrapped SEs are very similar to robust SE (above) and much higher than the original SE, illustrating the robust SE are a more accurate measure of the sample than than the original SE.
```{r}
#| label: bootstrap_SE

samp_distn<-replicate(5000, {  
  #take bootstrap sample of rows  
  boot_dat <- sample_frac(lmData, replace=T) 
  #fit model on bootstrap sample  
  bootfit <- lm(cases_last_7_days~posRate*confirmHosp, data=boot_dat) 
  #save coefs
  coef(bootfit) 
}) 

samp_distn %>% t() %>% as.data.frame() %>% summarize_all(sd)
```

  
  
---------------------------------------------
  


## Logistic Regression Models:

### Logistic Model with 2 predictors
#### Creating and Interpreting
Given the model's coefficients: for every 1 unit increase in `cases in the last 7 days` the odds of `highCases` incrase by 1.04, and for every 1 unit increase in `total deaths` the odds of `highCases` increase by 1.06
```{r}
#| label: two_pred_logistic
#| output: false

fit2 <- glm(
  highCases~cases_last_7_days+total_deaths,
  data=CovidData, 
  family="binomial")
summary(fit2)
```

```{r}
exp(fit2$coefficients)
```



#### Confusion Matrix
From the confusion matrix it appears the model is doing a decent job at classifying points with as little as 5.99% to 12.23% error.  
```{r}
#| label: confusion_matrix

CovidData <- CovidData %>% 
  filter( complete.cases(CovidData$highCases),
          complete.cases(CovidData$cases_last_7_days),
          complete.cases(CovidData$total_deaths) )

table(Predicted = (predict(fit2,type="response")>0.5) , 
      Actual = CovidData$highCases) %>% 
  as.data.frame() -> conf_mat

conf_mat <- conf_mat %>% 
  group_by(Actual) %>% 
  mutate(percent=Freq/sum(Freq))

ggplot(conf_mat) +
  aes(y=factor(Predicted,levels=c(F,T)),
      x=factor(Actual,levels=c(T,F)),
      fill=Freq,
      label=paste0(round(percent*100,2),"%") ) +
  geom_tile() +
  ggfittext::geom_fit_text(contrast = TRUE, reflow =TRUE) +
  scale_colour_discrete() +
  coord_fixed() + 
  xlab("Actual") + 
  ylab("Predicted") +
  theme_minimal()+ 
  theme(legend.position = "none")
```


The below table and provides diagnostic measures of the predictive power of the Logistic Model with 2 predictors.  
```{r}
class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if( is.numeric(truth)==FALSE & 
      is.logical(truth)==FALSE){
    truth<-as.numeric(truth)-1
  }
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(
    Accuracy = acc,
    Sensitivity = sens,
    Specificity = spec,
    PPV = ppv, 
    AUC = auc)
}

probs <- predict(fit2)
metrics <- class_diag(probs, CovidData$highCases)

output_2pred <- 
  metrics %>%
  t() %>%
  as.data.frame() %>%
  rename(`Value(2)` = "TRUE") %>%
  mutate(`Value(2)` = paste0(round(`Value(2)`*100,2),"%"))

kable(output_2pred)
```





#### Density Plot
This density plot illustrates the predictor value (logit) compared to the actual group membership to `highCases`.
```{r}
#| label: density_plot

densityVal <- CovidData %>% mutate(logit=fitted(fit2) )

ggplot(densityVal) + 
  aes(x=logit, fill=highCases) +
  geom_density(alpha=.5) +
  theme_minimal() + 
  labs(
    x = "logit",
    y = "Density"
  )
```




#### ROC and AUC
This ROC curve is of the logistic model with 2 predictors. Based on the shape of the ROC curve and the AUC of the ROC curve, the classification of this 2 predictor logistical model is very good
```{r}
#| label: ROC_AUC

ROCplot <- 
  CovidData %>%
  mutate(
    highCases = 
      case_when(
        highCases ~ 1,
        !highCases ~ 0,
        .default = NA
      )
  ) %>%
  ggplot()+
  aes(d=highCases,m=cases_last_7_days+total_deaths) +
  geom_roc(n.cuts=0)  +
  theme_minimal()

ROCplot

calc_auc(ROCplot)

```



### Logistic Model with all predictors
#### Creating and Interpreting
From including all variables the model was made more flexible, but had lower values for Accuracy, Sensitivity, and AUC, as there was more noise introduced.
```{r}
#| label: full_logistic

lassoData <- CovidData %>% 
  select(-fips, -county,-date) %>% 
  na.omit
fit3 <- lm(highCases~(.), data=lassoData)
probs <- predict(fit3,data=lassoData)
metrics <- class_diag(probs, lassoData$highCases)

output_all <- 
  metrics %>%
  t() %>%
  as.data.frame() %>%
  rename(`Value(All)` = "TRUE") %>%
  mutate(`Value(All)` = paste0(round(`Value(All)`*100,2),"%"))

kable(output_all %>% cbind(output_2pred))
```



#### 10-Fold Cross Validation (CV)
There is surprisingly no sign of overfitting the more flexible model by using all predictors. When having many degrees of freedom you can easily overfit the CV criteria for the model to be tuned to exploit the random variation in the dataset rather than improve predictive performance, which is what likely is occurring here.  To counteract this I will use LASSO will to reduce the degrees of freedom (number of variables used in prediction).
```{r}
#| label: CV_10fold
#| output: false

k=10

cvData<-lassoData[sample(nrow(lassoData)),] #randomly order rows
folds<-cut(seq(1:nrow(lassoData)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets  
  train<-cvData[folds!=i,]   
  test<-cvData[folds==i,]  
  truth<-test$highCases ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)  
  lassofit<-glm(highCases~(.),data=train,family="binomial")
  
  ## Test model on test set (fold i)   
  probs<-predict(lassofit,newdata = test,type="response")
  
  ## Get diagnostics for fold i  
  diags<-rbind(diags,class_diag(probs,truth))
}
```

```{r}
#average diagnostics across all k folds
summarize_all(diags,mean) %>%
  t() %>%
  kable()
```


#### LASSO
The variables that were selected by LASSO were those that increased the ability of the model to predict `highCases` and did not add extra noise in the data for the model to learn.  We can see that most of the variables were selected, so the CV critera was not overfit.   
```{r}
#| label: LASSO

library(glmnet)

y<-as.matrix(lassoData$highCases) #grab response
x<-model.matrix(highCases~(.),data=lassoData)[,-1] #grab predictors

cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)

var_to_select <- dimnames(coef(lasso))[[1]][coef(lasso)@i +1]


newLassoData <- lassoData %>% 
  mutate(regionNE=(region=="NE"))  %>% 
  mutate(regionSW=(region=="SW")) %>% 
  mutate(regionW =(region=="W" ))

lassoSelectedVarFunction <- highCases~
  regionNE+
  regionSW+
  regionW+
  cases_per_100k_last_7_days+
  total_cases+
  cases_pct_change_from_prev_week+
  deaths_per_100k_last_7_days+
  total_deaths+
  deaths_pct_change_from_prev_week+
  test_positivity_rate_last_7_days+
  total_tests_last_7_days+
  total_tests_per_100k_last_7_days+
  test_positivity_rate_pct_change_from_prev_week+
  total_tests_pct_change_from_prev_week+
  confirmed_covid_hosp_last_7_days+
  confirmed_covid_hosp_per_100_beds_last_7_days+
  confirmed_covid_hosp_per_100_beds_pct_change_from_prev_week+
  suspected_covid_hosp_last_7_days+
  suspected_covid_hosp_per_100_beds_last_7_days+
  suspected_covid_hosp_per_100_beds_pct_change_from_prev_week+
  pct_inpatient_beds_used_avg_last_7_days+
  pct_inpatient_beds_used_abs_change_from_prev_week+
  pct_icu_beds_used_avg_last_7_days+
  pct_icu_beds_used_abs_change_from_prev_week+
  pct_icu_beds_used_covid_avg_last_7_days+
  pct_icu_beds_used_covid_abs_change_from_prev_week+
  pct_vents_used_avg_last_7_days+
  pct_vents_used_abs_change_from_prev_week+
  pct_vents_used_covid_avg_last_7_days+
  pct_vents_used_covid_abs_change_from_prev_week

lassoSelectedVarFunction
```


#### CV with LASSO Variables
```{r}
#| label: LASSO_CV
#| output: false

######## IN-SAMPLE ########
fitLasso <- glm(lassoSelectedVarFunction, data=newLassoData)
probs <- predict(fitLasso)

output_lasso <- 
  class_diag(probs, newLassoData$highCases) %>%
  t() %>%
  as.data.frame() %>%
  rename(`Value(LASSO CV)` = "TRUE") %>%
  mutate(`Value(LASSO CV)` = paste0(round(`Value(LASSO CV)`*100,2),"%"))


######## OUT-OF-SAMPLE ########
k=10
cvData<-newLassoData[sample(nrow(newLassoData)),] #randomly order rows
folds<-cut(seq(1:nrow(newLassoData)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets  
  train<-cvData[folds!=i,]   
  test<-cvData[folds==i,]  
  truth<-test$highCases ## Truth labels for fold i

  lassofit<-glm(lassoSelectedVarFunction,data=train,family="binomial")  ## Train model on training set (all but fold i)  
  probs<-predict(lassofit,newdata = test,type="response") ## Test model on test set (fold i)   
  diags<-rbind(diags,class_diag(probs,truth))  ## Get diagnostics for fold i
}
```


```{r}
#average diagnostics across all k folds
output_lasso_cv <- 
  summarize_all(diags,mean) %>%
  t() %>%
  as.data.frame() %>%
  rename(`Value(LASSO CV)` = "V1") %>%
  mutate(`Value(LASSO CV)` = paste0(round(`Value(LASSO CV)`*100,2),"%"))

kable(
  output_lasso_cv %>% 
  cbind(output_lasso) %>%
  cbind(output_all) %>% 
  cbind(output_2pred)
)

```


Using the variables that LASSO selected, the model's out-of-sample AUC is higher than the model's in-sample AUC. The out-of-sample AUC (CV) is higher than the in-sample AUC (without CV).
